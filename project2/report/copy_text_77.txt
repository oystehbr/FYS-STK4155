\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{esint}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{ dsfont }
\usepackage{hyperref}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[font={small,it}]{caption}
\usepackage{caption}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage[euler]{textgreek}
\graphicspath{{./plots/}}
\usepackage{biblatex}
\addbibresource{reff.bib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[font=small,labelfont=bf]{caption}
\setcounter{secnumdepth}{5}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue
}

\title{Project 2 FYS-STK4155}
\author{Sigurd Holmsen, Øystein Høistad Bruce}
\date{November 2021}

\begin{document}

\maketitle

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{CASE.NM.BILDE.jpg}
\end{figure}

\newpage

\tableofcontents
\newpage

\section{Abstract}
% An abstract where you give the main summary of your work
A feed forward neural network is inspired by how a biological brain processes data, and is one of the most popular machine learning models. Our aim in this project is to write and present a neural network code, test it, and compare the results to other popular machine learning algorithms. In the last project we looked at linear regression, where we assumed a specific model (polynomials) to apply to a data set. Neural networks have the advantage that they do not need any assumptions of the complexity of the data, but we can feed data directly into the model and usually end up with a good fit. We will show some of the advantages/disadvantages of neural networks compared to other machine learning methods.

\section{Introduction}
%Explain neural network, modelled after neurons in biology, motivate!
After we have explained and written a neural network code, we want to test both how well it is able to predict data and how quickly it can be trained. We will adapt the model to both a linear regression case and a binary classification case. We will do this in several steps. \\

\noindent
First, we need to program a gradient descent algorithm, and we will be using a stochastic gradient descent to save computing time. A gradient descent uses gradients of a given cost function to optimize the model parameters. We will also chose a momentum based gradient descent which uses the steepness of the gradient to adjust the length of each step. The stochastic gradient descent chooses a random subset of the data to compute gradients, so that we do not need to use the entire data set. \\

\noindent
Next up, the neural network class. We implemented a code that where able to initialize and train a feed forward neural network. We are going to integrate the stochastic gradient descent algorithm as the method for tuning the weights and biases inside the model. We will analyse both a regression- and classification problem with the neural network and compare the results against the linear- and logistic regression methods (respectively). \\

\noindent
In this report, we are going to look over the methods we have been implementing in this project. Then, an introduction to the result section with some general information about the upcoming analysis, followed by the actual analysis and discussion. In the end, we will conclude the project with some learning outcomes and further analysis we would like to do. \\

\noindent
We will introduce the data in the sections \ref{sec_regression_data} and \ref{sec_classification_data} for the regression- and classification case (respectively). The code is stored in the github repository attached at the last page of the report (section \ref{sec_github_repository}).


\section{Method}
Here, we explain the functions we have programmed and some concepts behind them. The functions take in input data $X$ and output data $y$, so data must be generated or extracted from somewhere before the models can be used. 
%TODO: explain assumptions for all models and data 

\subsection{Gradient Descent}
The goal of gradient descent is to find parameters that minimizes a cost function. We have written a gradient descent function that takes in initial values of the parameters that will be tuned, a cost function, the learning rate $\eta$, a hyper-parameter $\lambda$ and the number of iterations, as well as input and output data $X, y$. The learning rate $\eta$ decides the length of each step in the search direction. In this method, we have used the python package autograd for finding the gradients of the cost function. We then use the gradients and the learning rate to converge towards the minimum of the cost function, after which the function returns the updated/ tuned parameters that's defining the model. The algorithm we used is explained in the next subsection. 

\subsubsection{Momentum}
We have used a momentum based gradient descent, which means the length of each step in the gradient descent depends on the size of the previous gradient. The point is to do longer steps when gradients are steeper, and smaller steps when the gradients are flatter. In this way, the gradient descent adjusts to the "terrain" of the cost function. The algorithm can be expressed as follows: 

\begin{align}
    v_t &= \gamma v_{t-1} + \eta_t \frac{d C}{d \theta} \\
    \theta_{t+1} &= \theta_t - v_t
\end{align}

where the parameter $\gamma \in [0, 1]$ is the amount of momentum we want to use (no momentum is 0). $v_t$ is the amount of change in the tuning parameter at time $t$. $\theta$ is the parameter we want to tune, and $\frac{d C}{d \theta}$ is the change in cost with respect to $\theta$.

\subsubsection{Stochastic}
We have made our gradient descent method allowing stochastic, meaning we do not necessarily compute the gradients for the entire data set $X, y$, but for a small, randomly chosen batch $X_k, y_k$. If we let the size of the mini-batches to be $M$, then we will have $m$ mini-batches, where $m=\frac{\text{number of data}}{M}$ (rounded down to nearest integer). We then select a random integer $k\in [0, 1, ... (m-1)]$, and select the k-th mini-batch:

\begin{align*}
    \text{}
    X_k &= X[k \cdot M:(k+1) \cdot M]\\
    y_k &= y[k \cdot M:(k+1) \cdot M]
\end{align*}
In this way, we have produced a randomly chosen mini-batch from $X, y$, and computed the gradient $\frac{\text{number of data}}{M}$ times. We also decide the number of epochs, that is how many times we will repeat this process. The goal of the stochastic gradient descent is to save computational power and time, as we compute fewer gradients. 

\subsection{Neural Network}
\subsubsection{Architecture}
We have made a neural network class. Regarding the network architecture, the class takes the following parameters: number of input nodes, number of output nodes and a list of hidden layers with corresponding nodes. Each element of the list of hidden layers is an integer representing the number of nodes of the related layer. This allows us to set up the network architecture however we would like, where number of nodes per layer can vary. For more information on how to initiate a class, please look at the README file. 
\\ \\
Then we initialize the weights using the normal distribution and the biases as small float numbers. The class also has parameters for the activation function for the hidden layers and for the output layer, where the sigmoid function is the default activation function. Then, the user can choose the values for the stochastic gradient descent. That includes the learning rate $\eta$, the regularization parameter $\lambda$, batch size and number of epochs in the SGD, and the hyper-parameter $\gamma$.
\\ \\
The program does not scale data, so this must be done beforehand if needed. For more information on how the code works, look at the comments and doc-strings in the code. 

\subsubsection{Regularization parameters}
Our network takes a regularization parameter $\lambda$ which adds a term to the cost function:
$\lambda  \left\lVert w \right\rVert^2_2$. This term is added in attempt to avoid an overfit of the model. This is implemented as a parameter in the Neural Network class as explained above. 
% TODO: vanishing gradients
% TODO: exploding gradients

\subsubsection{Hyperparameter}
The network has also a method for choosing the hyperparameter $\gamma$, which decides the amount of momentum applied to the gradient descent. This may help finding a minimum quicker. This is implemented as a parameter in the Neural Network class as explained above. % TODO: maybe refer to github instead?

\subsubsection{Cost Function}
When a neural network is initialized, one has to decide which cost function to use, as this will influence the gradient descent, or more specifically the back propagation algorithm. The cost function must be given in a method inside the class. It is important that the cost function can be interchanged, as the linear regression and classification cases use different cost functions. 

\subsubsection{Feed Forward}
The feed-forward function takes a set of data and predicts the output given the model weights and biases. The data should be a matrix of dimension $(\#data, \#features)$. This means we are approximating several output values simultaneously. Each node is connected to all the nodes in the next layer, and every node has its own bias. After each node has received values from the previous nodes, multiplied with the weight matrix and added with the bias, it it sent through the hidden layer activation function. This value is sent to the nodes in the next layer, and this process is repeated until the output node(s) are reached. The output node(s) may have a different activation function than the hidden layers, or it may not have an activation function at all. 

\subsubsection{Activation functions}
The activation functions are used in the feed forward part underneath, the different activation function we are looking at in this project are:\\

\\\noindent
\textbf{Sigmoid}
\begin{equation}
    f(z) = \frac{1}{1+\exp{-z}}
\end{equation}

\\\noindent
\textbf{Rectified Linear Unit (RELU)}
\begin{equation}
    f(z) = 
        \begin{cases}
            \, z, & z > 0\\
            \, 0 & \text{otherwise}
        \end{cases}
\end{equation}

\\\noindent
\textbf{Leaky Rectified Linear Unit (Leaky RELU)}
\begin{equation}
    f(z) = 
        \begin{cases}
            \, z, & z > 0\\
            \, -\alpha z & \text{otherwise}
        \end{cases}
\end{equation}

for some (small) $\alpha > 0$\\

\subsubsection{Back Propagation}
% TODO: do we need to explain the SGD again?
To tune our weight and bias parameters, we use a stochastic gradient descent (SGD). That means we randomly pick a batch from $X$, and the corresponding output values from $y$, and compute the gradients of the parameters from these. We use a momentum based SGD. To find the gradients for the weights and biases, we perform the back propagation algorithm. First, we choose a cost-function, for instance a variant of the mean square error, as a measurement of the quality of our model:

\begin{equation}
    MSE = \frac{1}{2n} \sum_i (y_i - \tilde{y}_i)^{2}
\end{equation}
Then we are interested in finding the gradients for a given layer $l$:

\begin{equation}
    \frac{\delta C}{\delta w^l_{ij}}, \, \frac{\delta C}{\delta b^l_j}
\end{equation}
According to the back propagation algorithm, this can be written as:

\begin{align}
    \frac{\delta C}{\delta w^l_{ij}} = \delta_j^l a_k^{l-1}, \, \frac{\delta C}{\delta b^l_j} = \delta_j^l
\end{align}

where

\begin{equation}
    \delta_j^l = \sum_k \delta_k^{l+1} w_{kj}^{l+1} f'(z^l_j)
\end{equation}
Here, $f$ is the activation function for the given layer. We can compute this if we know the value of $\delta^{l+1}$. We can compute, the last layer delta in such way:

\begin{equation}
    \delta^L = \frac{1}{2n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\end{equation}
So, we are able to find all the $\delta$ values starting backwards. After we have found all the gradients for both the weights and biases, we use this for the momentum based SGD in the same way as described in the section above. When the SGD has iterated through all the given epochs, the training of the model is complete. 

\subsubsection{Regression case}
% TODO: Scaling data may be smart to not set exploding gradients after first iteration (with random weights)
For the regression case, we will not need an activation function for the output layer. This is because we may want to fit any number from $-\infty$ to $\infty$, and then we do not want to put a boundary on the value of the output. We also use the MSE as the cost function, which is accounted for in the back propagation algorithm.
% TODO: the MSE or a variant of it

\subsubsection{Classification case}
% TODO: Scaling the data 
% Scaling data may be smart to not set exploding gradients after first iteration (with random weights)
For the classification case, we want the output to be interpreted as probabilities. Hence we want to limit the output between 0 and 1. Since we are studying a binary classification problem, we use the sigmoid function as the output layer activation function. We have used the log-loss cost function when deriving the back propagation algorithm:

\begin{equation}
    C(W) = - \sum_{i=0}^n y_i log(\hat{y}_i) + (1-y_i) log(1 - \hat{y}_i)
\end{equation}

\noindent
When using the neural network, we have used cancer data from the Wisconsin studies, and have utilized principal component analysis to rule out the two most important features from the data set. Principal component analysis looks at the unit vectors (representing the features) with the greatest eigenvalues, and we then only send the chosen data into our neural network for the training. 

\subsection{Logistic Regression}
When performing the logistic regression, we have used the following cost function: 

\begin{equation}
    \prod_{i=0}^n \, p(x_i=1 | \beta)^{y_i} \, p(x_i=0|\beta)^{1-y_i}
\end{equation}

Where
\begin{equation}
    p(x=1|\beta) = \frac{e^{\beta_0 + \beta_1x_1 + ... + \beta_nx_n}}{1 + e^{\beta_0 + \beta_1x_1 + ... + \beta_nx_n}}
\end{equation}
In this equation, the $\boldsymbol{\beta}$ is the parameter we want to train and $\boldsymbol{x}$ is the input features and $n$ is the number of input features. In the final model, the probability greater than  $0.5$ will refer to a prediction of $1$ and otherwise the model will be predicting $0$. \\

\noindent
To train our model, we will perform the SGD as before, where we will use the autograd package to find the gradients of the cost function. 


\newpage
\section{Results and discussion}
\subsection{Introduction to the results}
In this section, we will refer to some tests we have made in the \textbf{test\_project\_2.py} which will be found in the \textbf{project2} folder inside the github repository, attached at the end of the report. To have a great experience with the testing file, we recommend you to read the \textbf{README.md} file inside the \textbf{project2} folder. We will provide the correct parameters for doing the tests while we present the results.

\subsubsection{General comments on the results}
Here is a list of things to be aware of according to the results we are presenting:
\begin{enumerate}
    \item When we are comparing two parameters, we looking at the R2-score or accuracy score to the regression- and classification problem (respectively). Among some great results there will be some \textit{wierd} results also (even with a small change of a parameter value). This is often some case where the model are going to predict the same target value for every kind of input value. Such cases can maybe be some \textit{local} minimum of the cost-function. We have not \textit{fixed} those occurrences, but we know about the problem and after some testing it would help to re-initializing the weights and biases. TODO: finish this
    \item In our analysis, we had to create up some starting values for the parameters we wanted to tune. Whenever we got out some results from a previous testing, we added the greatest parameters to the new testing. TODO (maybe): it would probably be great to go over more starting values and do the fully process on those (and then compare) - TODO (kan juge her), we have done it and the starting values is chosen in such way. 
    \item The number of epochs we have chosen in the analysis isn't optimized in some way. Greater iterations will, very often, get to a better model, as a result we have chosen to go with some \textit{friendly} number of epochs to save computing expenses. 
\end{enumerate}
% TODO: Explain usage of n_epochs -> computing power -> more iterations -> better results. n_epochs = 300 (this i picked by have many iterations, but also computational (often)
% TODO: Maybe show of some plot 

\subsubsection{Introducing the data - regression case} \label{sec_regression_data}
For the testing of the regression case, we have generated data from the Franke function similarly as in project 1. That means we generate $x, y$ -pairs, both randomly selected between 0 and 1, and send them into the Franke function. A noise term is then added, which is a normally distributed value with a noise multiplier parameter $noise$. \\

\noindent
We have scaled the data when we compare the results from project 1 with the results from the $SGD$-algorithm. We are doing that because the code from project 1 does so. We do not find it necessary to scale the data in further/ other analysis with the Franke function data, since the input values are restricted between 0 and 1. \\

\noindent
In our analysis, we have looked at a dataset consists of 100 datapoints, and a noise of 0.01.

\subsubsection{Introducing the data - classification case} \label{sec_classification_data}
For the classification case, we have used the Wisconsin cancer data found in the Sci-kit learn package $sklearn.datasets$. The data consist of 569 observations and 30 features. If you have much data, then you use all of it to create a best possible model. We are no exception, and we will consider all the observations in our analysis.\\

\noindent
We only need to consider two features from the data set, and that is because we are able to predict very well with just those two. We are choosing them optimally by looking up the two most important features according to the principal component analysis (PCA), where the two most important features are having a total \textit{explained variance ratio} above $0.99$ (which is really high). \\

\noindent
We scale the data by the maximum of each input feature, so that the largest input value will be 1 for each feature.

\subsubsection{Initializing the weights and biases}
We are initializing the weights according to a standard normal distribution and all the biases are valued 0.01 before it starts to optimize these parameters. We are initializing the biases at 0.01 since it ensures that all neurons have some output which can be backpropagated in the first training cycle.\\

\noindent
We have looked at the possibility of refreshing the weights and biases when the predictions goes horrible, often when the model is predicting the same targets for all kind of input, but were told under the project that this wasn't a great way to fix the problem. Those occasions would we refer to the different parameters as unstable, and we should not handle this problem inside the code. 
% TODO: How would you initialize the biases
 

\subsection{Regression problem}
\subsubsection{OLS - results of Stochastic Gradient Descent}

For the OLS regression, we have compared the R2-score for different learning rates and number of minibatches. We want to check how far we are from the analytical solution from project number 1. The following values are being set (and are looking at the Franke Function data, as mentioned above):
\begin{lstlisting}[language = Python]
----------------------------------------------------------
number_of_epochs = 200
degree = 5
gamma = 0.7
run_main_OLS = True
list_number_of_minibatches = [1, 5, 8, 10, 16, 20, 40, 80]
----------------------------------------------------------
\end{lstlisting}
In this case, we want to look at the number of iterations in the SGD-algorithm to be the same for different number of minibatches, such that the values will not be affected that much from the big increasing of iteration by increasing the number of minibatches. 
With these values, we can turn in to the \textbf{test\_project\_2.py} file, and go into \textbf{test 1}. When we run the code, we will be provided with these figures (fig. \ref{fig: regression_OLS_main_batch_eta_training} and \ref{fig: regression_OLS_main_batch_eta_test}).

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{test1_OLS_gamma_0.7_training_11.png}
    \captionof{figure}{Data: training, R2-score of combination of number of minibatches and the learning rat}
    \label{fig: regression_OLS_main_batch_eta_training}
\end{figure}

\noindent
We observe that the optimal learning rate is around $0.1$ and $0.01$ for this case, as this gives the highest R2-score. As a result of that, it will be the closest one to the analytical solution. When we run \textbf{test 1}, we will also be informed, with a print in the terminal, that the R2-score for training and testing is around 0.982 and 0.964 (respectively). Which is not that far away from the results we are getting in these numerical approaches, but with a greater number of iteration we would probably get even closer (as we will see in its own analysis).

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{test1_OLS_gamma_0.7_test_11.png}
    \captionof{figure}{Data: test, R2-score of combination of number of minibatches and the learning rate}
    \label{fig: regression_OLS_main_batch_eta_test}
\end{figure}

\noindent\\
If the mini-batch size is too small (equivalent that the number of minibatches is to high), there may be too much randomness in the computed gradients to converge quickly, and if the size is too large, the problem of computing time arises, which defeats the purpose of the stochastic gradient descent. In our case, we do not see that much difference in the R2-score for different batch sizes (TODO: why). Hence, since the mini batch size do not affect the R2-score it would be a great idea to choose the smallest batch size faster computing time. \\

\noindent
\\Now, we want to show that higher $number\_of\_epochs$-value leads to better results (if the solution seems to converge). The following two figures (fig. \ref{fig: regression_OLS_main_batch_eta_test_10} and \ref{fig: regression_OLS_main_batch_eta_test_1}), showing only the data set for testing, are showing that the statement is true. In these figures, we are looking at the same problem as above - but with these values (respectively):

\begin{lstlisting}[language = Python]
--------------------------------
number_of_epochs = 10
number_of_epochs = 1
--------------------------------
\end{lstlisting}
every other value is sat the same as in the previous testing, so we are only interested in looking at the results with respect to the $number\_of\_epochs$. Again, we are looking at \textbf{test 1} in the \textbf{test\_project\_2.py} file. 

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{test1_OLS_gamma_0.7_epochs_10_test_12.png}
    \captionof{figure}{Same as figure \ref{fig: regression_OLS_main_batch_eta_test}, but $number\_of\_epochs = 10$}
    \label{fig: regression_OLS_main_batch_eta_test_10}
\end{figure}

\noindent
By looking at figure \ref{fig: regression_OLS_main_batch_eta_test}, \ref{fig: regression_OLS_main_batch_eta_test_10} and \ref{fig: regression_OLS_main_batch_eta_test_1} we can confirm that higher number of iterations in the SGD-algorithm will lead to a better result. Then a question arises, why do we not increase the number of epochs to infinity? The answer is simple, the computing time will be increased as the number of epochs increases. Therefore, we need to restrict the number of epochs for us to be able to move forward with the analysis. So, in later analysis we have not optimized the number of epochs in some way, since it is not optimizable. Therefore, we have often picked the parameter so the runtime is not too large. 

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{test1_OLS_gamma_0.7_epochs_1_test_12.png}
    \captionof{figure}{Same as figure \ref{fig: regression_OLS_main_batch_eta_test}, but $number\_of\_epochs = 1$}
    \label{fig: regression_OLS_main_batch_eta_test_1}
\end{figure}

\paragraph{Scaling the learning rate}
\\\noindent
We want to study an algorithm for scaling the learning rate. We have tried the following algorithm:

\begin{equation}
    f(t) = \frac{5}{t + 50}
\end{equation}

\noindent
This algorithm scales the learning rate to be smaller over time. We compare the results with and without the algorithm, where we run five tests both with and without the scaling algorithm. You can find this test by navigate in the \textbf{test\_project\_2.py} file to \textbf{test 13}. We use these values for testing:

\begin{lstlisting}[language = Python]
-----------------
n_epochs = 200
gamma = 0
eta = 0.1
batch_size = 10
-----------------
\end{lstlisting}
\noindent
This would give out a result in this form:

\newpage
\begin{lstlisting}[language = Python]
----------------------------------------
With Scaling learning rate algorithm 
Test 1: MSE: 0.189056
Test 2: MSE: 0.189427
Test 3: MSE: 0.189085
Test 4: MSE: 0.188920
Test 5: MSE: 0.188831
Without Scaling learning rate algorithm
Test 1: MSE: 0.184023
Test 2: MSE: 0.183986
Test 3: MSE: 0.184069
Test 4: MSE: 0.183949
Test 5: MSE: 0.184023
----------------------------------------
\end{lstlisting}

\noindent
Since we do not observed an improved MSE, we do not use the scaling algorithm for the rest of the project as this saves time and may also lead to better results. We already have a momentum based SGD, so perhaps the scaling algorithm is somewhat redundant. 

\subsubsection{RIDGE - results of Stochastic Gradient Descent}
We make a graph comparing the R2-score of our model as a function of learning rate and hyper-parameter $\lambda$ for Ridge regression.\\

\\\noindent
We observe that the results are sensitive to the learning rate $\eta$, based on figure \ref{fig: regression_RIDGE_main_lmbda_eta_test}. A too large learning rate may cause the gradient descent to miss the global minimum we are trying to find, while a too small learning rate may not converge quickly enough to the minimum. When comparing different learning rates to each other, we have used static learning rates to better see the difference in results. We also see that a too high hyper-parameter $\lambda$ will unnecessarily punish good fits while too low $\lambda$ will fail to punish overfitting. The optimal choice seems to be $\lambda = 10^{-5}, \eta = 1$. Note that when attempting to use higher learning rates, the program crashed as our parameters diverged. We can clearly see that the accuracy of the model is more sensitive to the learning rate $\eta$ than the hyper-parameter $\lambda$. This indicates that we should be careful when choosing the learning rate, as it may greatly change the resulting model.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{test1_nepochs_200_M_100_gamma_0_numdata_500_noOfMinibatches_10_degree_4_test.png}
    \captionof{figure}{Plot of the R2-score of the model as function of learning rate $\eta$ and hyper-parameter $\lambda$. We have used a degree of 4 for the polynomials, $batch\_size = 10$, $number\_of\_epochs = 200$}
    \label{fig: regression_RIDGE_main_lmbda_eta_test}
\end{figure}

\noindent
Then, we find the best mini-batch size. We run a test comparing the MSE for different sizes:

\begin{lstlisting}[language = Python]
--------------------------------------------
MSE: 0.232627 (NUMERICAL (batch size = 1))
MSE: 0.232035 (NUMERICAL (batch size = 5))
MSE: 0.231755 (NUMERICAL (batch size = 10))
MSE: 0.231771 (NUMERICAL (batch size = 30))
MSE: 0.231771 (NUMERICAL (batch size = 60))
MSE: 0.231758 (NUMERICAL (batch size = 100))
--------------------------------------------
\end{lstlisting}

\noindent
We notice that a batch size of 10 gives the lowest MSE, hence this is the optimal choice for the ridge regression. Since there are 80 data points for training (100 in total), a batch size of 10 gives out 8 mini batches. 

\newpage
\subsubsection{Neural network}
% TODO: order: architecture, batch_size and gamma, lmbda and learning rate
%Mostly problem b
%Analysis of the regularization parameters (task b)
%Seaborn
We will use our neural network to predict the Franke function as in project 1. How we extracted our data is described in \ref{sec_regression_data}. To make the model work as well as possible, we have to choose the correct parameters. With the Franke function data set, we will not set any activation function for the output layer, since we have not scaled the data in this case. That is because our neural network is able to handle all values, not just values between 0 and 1. We have chosen to start by studying with the Sigmoid as an activation function for the hidden layers. \\

%seaborn - comment too high or too low parameters
\\\noindent
\textbf{First, we need to find the optimal architecture of the neural network}\\
\\We are finding the optimal architecture by looking at a seaborn plot with the number of layers and nodes at the x- and y-axis (respectively). To be able to create such a plot, we need to initialize some parameters. We started by setting the parameters in the following way:

\begin{lstlisting}[language = Python]
-----------------
n_epochs = 300
batch_size = 10
gamma = 0
lmbda = 0.0001
eta = 0.001
-----------------
\end{lstlisting}
With these values, we received the following plots as shown in figure \ref{fig: regression_architecture_training} and figure \ref{fig: regression_architecture_testing}, by using \textbf{test 9} in \textbf{test\_project\_2.py}. The plot tells us how we should build up our neural network by picking some architecture, a combination of the amount of nodes and layers, that gives us the highest R2-score. \\

\\\noindent
By looking at the figures \ref{fig: regression_architecture_training} and \ref{fig: regression_architecture_testing}, we can observe that the amount of nodes need to be (much) greater than the number of hidden layers for the neural network to be good as a model. We can also see that among the cells with the highest R2-score, there will be some "bad" ones also. Those cells (with R2-score around 0), have gone to a local minimum - where the model is predicting the same target values for all kind of input data. It is possible to avoid many of such occasions, by refreshing the weights and biases (start the algorithm over again), but it looks like we shouldn't do so (TODO). We tried to find a region where the architecture was kind of stable and worked good as a model, those values were 40 hidden nodes and 3 hidden layers. So, from now on we are going to evaluate the remaining parameters given this architecture.\\
\\\noindent
A too high complexity of the model will be more computationally expensive, and may also result in an overfit, while a too simple model seems to give an underfit.

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test9_M_10_gamma_0_lmbda_0.0001_eta_0.001_training_1.png}
    \captionof{figure}{Data: training, R2-score of different architecture of the neural network}
    \label{fig: regression_architecture_training}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test9_M_10_gamma_0_lmbda_0.0001_eta_0.001_test_1.png}
    \captionof{figure}{Data: testing, R2-score of different architecture of the neural network}
    \label{fig: regression_architecture_testing}
\end{figure}


\\\noindent
\textbf{Secondly, we need to find the optimal batch size and momentum parameter}\\
\\Now we want to find the optimal batch size and momentum parameter by looking at a seaborn plot with the momentum parameter and batch size at the x- and y-axis (respectively). To be able to create such a plot, we need to  initialize some parameters (again). Now, we have optimized the architecture of the neural network (by last interpretations), and will be using those as initial values. So, the parameters we now set are:

\begin{lstlisting}[language = Python]
------------------
node_list = [40]*3
n_epochs = 300
lmbda = 0.0001
eta = 0.001
------------------
\end{lstlisting}
If we now insert those parameter values inside \textbf{test 10} in \textbf{test\_project\_2.py}, then we will get two plots (figure \ref{fig: regression_batchSize_gamma_training} and figure \ref{fig: regression_batch_Size_gamma_testing}). By looking at the plots, it seems like the most "stable" values (the models that are not predicting the same output) lies in the middle of the plot. We can see the same trend in the R2-score for both training and testing data. So, we have chosen to go further with a batch size of 16, and a gamma value of 0.4


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test10_lmbda_0.0001_eta_0.001_hiddennodes_40_hiddenlayer_3_training_1.png}
    \captionof{figure}{Data: training, R2-score of different batch size and gamma values}
    \label{fig: regression_batchSize_gamma_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test10_lmbda_0.0001_eta_0.001_40_hiddenlayer_3_test_1.png}
    \captionof{figure}{Data: testing, R2-score of different batch size and gamma values}
    \label{fig: regression_batch_Size_gamma_testing}
\end{figure}

\\\noindent
\textbf{Last, we need to find the optimal hyperparameter lambda and learning rate}\\
% TODO: didn't finish this!!!!
\\Now we want to find the optimal hyperparameter lambda and learning rate by looking at a seaborn plot. Now, by the parameters we found for the batch size, momentum (in the SGD) and the architecture of the neural network, we are able to use those values for finding the optimal lambda and eta values. We will try to find the parameters, $\eta$ and $\lambda$, with the greatest R2-score. We are setting the parameters in the following way:
\begin{lstlisting}[language = Python]
------------------
node_list = [40]*3
n_epochs = 2000
batch_size = 16
gamma = 0.4
------------------
\end{lstlisting}
If we now insert those parameter values to \textbf{test 4} in \textbf{test\_project\_2.py}, then we will get two plots (figure \ref{fig: regression_lmbda_eta_training} and figure \ref{fig: regression_lmbda_eta_testing}). The figures tells us that the best learning rate, eta, is around 1e-3 and 1e-4 and the most stable lambdas are between 1e-3 and 1e-6. 


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test4_M_16_gamma_0.4_hiddennodes40_hiddenlayers_3_training_1.png}
    \captionof{figure}{data: training, R2-score of different lambda and eta values}
    \label{fig: regression_lmbda_eta_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test4_M_16_gamma_0.4_hiddennodes40_hiddenlayers_3_test_1.png}
    \captionof{figure}{data: training, R2-score of different lambda and eta values}
    \label{fig: regression_lmbda_eta_testing}
\end{figure}

\noindent
We can see at fig. \ref{fig: regression_lmbda_eta_training}, \ref{fig: regression_lmbda_eta_testing} that for a greater lambda value, the model will not be as overfitted as it would be without such a parameter. Also, we observe that with a greater learning rate than 1e-3 we will be in a difficult situation where the model will predict totally wrong, where the tuned parameters (weights, biases) have probably got some exploding- or vanishing gradients (TODO). With a too low learning rate, we observe that in some cases the training have gone too slow to reach some great tuned parameters. This could be fixed by setting up the number of epochs, but this would require more computing power which is something we want to avoid if possible. The optimal parameters in this case are: $\eta = 10^{-3}$, $\lambda = 10^{-4}$, $\gamma = 0.4$, batch size = 14, layers = 4, nodes in hidden layers = 40.


\paragraph{Different activation functions}
So, the fully analysis we have done so far was with the Sigmoid as the activation function for the hidden layer. Now, we are ready for looking at some other activation functions. We have gone into details in the Sigmoid case, and will just briefly comment the optimization of the coming activation functions
% TODO: tell it in the beginning of the section

\subparagraph{the RELU}
To study the RELU function, we find all other parameters again. We find the optimal degrees, nodes, $\gamma$, batch size, $\eta$ and $\lambda$ again:

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test9_M_10_gamma_0_lmbda_0.0001_eta_0.001_test_layerNameRELU_1.png}
    \captionof{figure}{Studying the R2-score with the RELU activation function for different layers and number of nodes}
    \label{fig: regression_architecture_RELU_testing}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{pro.png}
    \captionof{figure}{Studying the R2-score with the RELU activation function for different batch sizes and $\gamma$ values}
    \label{fig: regression_batchSize_gamma_RELU_testing}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test4_M_6_gamma_0.6_hiddennodes8_hiddenlayers_4_test_1.png}
    \captionof{figure}{Studying the R2-score with the RELU activation function for different learning rates and $\lambda$}
    \label{fig: regression_lmbda_eta_RELU_testing}
\end{figure}

We note that when the learning rate was higher than $10^{-3}$, the code did not run, presumptuously due to exploding gradients. It appears that the optimal parameters for the RELU activation function are $\eta = 10^{-3}, \lambda = 10^{-5}, \gamma = 0.5$, batch size = 6, number of layers = 4, nodes per layer = 8.

According to fig.\ref{fig: regression_lmbda_eta_RELU_testing}, we achieve an R2-score of 0.87 for the testing data. Compared to the same tests for the sigmoid function, the RELU activation function does not seem to tolerate many hidden layers and many nodes, as this may cause gradients to vanish or explode. We sometimes get Runtime Warnings for underflow from python which indicated that gradients are vanishing, which can be expected of the RELU function as the gradient is zero for certain input values. We get a slightly higher R2-score with the RELU than the sigmoid, meaning RELU might be the better choice for the data we are working with. 


\subparagraph{Leaky RELU}

To study the leaky RELU, we find all the optimal parameters using the leaky RELU activation function for the hidden layers:

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test9_M_10_gamma_0_lmbda_0.0001_eta_1e-05_test_layerNameLeaky_RELU_1.png}
    \captionof{figure}{Studying the R2-score with the Leaky RELU activation function for different amount of layers and nodes}
    \label{fig: regression_lmbda_eta_RELU_testing}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test10_lmbda_0.0001_eta_0.0001_8_hiddenlayer_4_hidden_layer_name_Leaky_RELU_test_2.png}
    \captionof{figure}{Studying the R2-score with the Leaky RELU activation function for different $\gamma$ values and batch sizes}
    \label{fig: regression_lmbda_eta_RELU_testing}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test4_M_5_gamma_0.6_hiddennodes8_hiddenlayers_4_name_hidden_Leaky_RELU_test_2.png}
    \captionof{figure}{Studying the R2-score with the Leaky RELU activation function for different learning rates $\eta$ and $\lambda$ values}
    \label{fig: regression_lmbda_eta_RELU_testing}
\end{figure}

\noindent
We conclude that the following parameters seem optimal for the Leaky RELU activation function: $\eta = 10^{-3}, \lambda = 10^{-5}, \gamma = 0.6$, batch size = 5, number of layers = 4, nodes per layer = 8. For these parameters we achieve an R2-score of 0.82 for the test-data, slightly less than for the RELU activation function.\\

\noindent
The results look similar to the ones for the RELU function, but the leaky RELU seems more stable, as the program seems more stable and we do not get Runtime Warnings from python because of disappearing gradients. 

\subparagraph{Evaluation}
We observe that all the activation functions achieve roughly the same R2 scores. The computing time is also similar. Meanwhile, one has to be more careful when choosing parameters for the RELU function, as the gradients can more easily vanish which causes a terrible model. 

\subsubsection{Comparison: Neural network and linear regression}
%Observe: the neural network may depend on the inital weights and biases
%Linear regression not good for tuning many variables
We compare out neural network to our code from project 1, i.e. the normal linear regression with an analytical solution. We are still using the data form the Franke function. We compare both R2-scores and the running time of each method. We run the test with the optimal parameters for the sigmoid function as chosen in section 4.3, and the test can be reproduced by running \textbf{test 3} in our test file:

\begin{lstlisting}[language = Python]
----------------------------------------
> Neural Network (time spent: 12.7519s):
** (R2) TRAINING DATA: 0.822
** (R2)  TESTING DATA: 0.7969

> OLS (time spent: 0.0005s):
** (R2) TRAINING DATA: 0.9724
** (R2)  TESTING DATA: 0.9636

> RIDGE (time spent: 0.0003s):
** (R2) TRAINING DATA: 0.9201
** (R2)  TESTING DATA: 0.8661
----------------------------------------
\end{lstlisting}

\noindent
We observe that in this case, our neural network does not only achieve a lower R2-score than both the OLS and Ridge regression methods, it also much slower. The reason for this is probably that we are only using 2 input variables (as in project 1), and the matrix inversion performed in OLS and Ridge regression does not require heavy computation. If we instead had looked at a data set with hundreds of input variables, the neural network would most likely be a better model as it does not suffer as badly from the curse of dimensionality as matrix inversion. 


\subsection{Classification problem}
The data set we are using in the classification problem is described in section \ref{sec_classification_data}.
\subsubsection{Neural network}
We will use our neural network to predict 0 and 1's of the \textit{Wisconsin Breast Cancer} data set. Since we are looking at targets with 0 or 1, it will be a much greater idea to choose some kind of an activation function for the output layer too (as well for the hidden layers). In the coming analysis, we have chosen to start looking at the activation function Sigmoid for both hidden- and output layer(s). To make a great model, we need to find the greatest hyperparameters. \\

\\\noindent
\textbf{First, we need to find the optimal architecture of the neural network}\\
\\We are finding the optimal architecture by looking at a seaborn plot with the number of layers and nodes at the x- and y-axis (respectively).To make a great model, we need to find the greatest hyperparameters. We will start out search of a great architecture with these parameters. 

\begin{lstlisting}[language = Python]
--------------------------------
n_epochs = 300
batch_size = 10
gamma = 0
lmbda = 0.0001
eta = 0.001
--------------------------------
\end{lstlisting}


\noindent
With these values, we received the following plots as shown in figure \ref{fig: classification_architecture_training} and figure \ref{fig: classification_architecture_testing}, by using \textbf{test 11} in \textbf{test\_project\_2.py}. The plot tells us how we should build up our neural network by picking some architecture, a combination of the amount of nodes and layers, that gives us the highest accuracy score. \\


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test11_M_10_gamma_0_lmbda_0.0001_eta_0.001_hidact_sigmoid_outact_sigmoid_training_6.png}
    \captionof{figure}{Data: training, accuracy of different architecture of the neural network}
    \label{fig: classification_architecture_training}
\end{figure}

\\\noindent
By looking at the figures \ref{fig: classification_architecture_training} and \ref{fig: classification_architecture_testing}, we do observe some similarities with the regression case, that the number of nodes need to be greater than the number of hidden layers for the model to predict well. We will again find some architecture that is kind of stable, in the sense of slightly different parameters will not predict much worse, but also try to find the combination of nodes and layers that will achieve the highest accuracy score. 
\\\noindent

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test11_M_10_gamma_0_lmbda_0.0001_eta_0.001_hidact_sigmoid_outact_sigmoid_test_6.png}
    \captionof{figure}{Data: testing, accuracy of different architecture of the neural network}
    \label{fig: classification_architecture_testing}
\end{figure}
\noindent
Fig. \ref{fig: classification_architecture_training} and \ref{fig: classification_architecture_testing} are telling us that the number of nodes and layers should be about 20 and 1 (respectively). So, we are going to use those values in the further analysis of the other parameters we want to tune. We pick 20 as the number of hidden nodes because the lower amount of nodes, the more \textit{computing friendly it gets}.\\

\\\noindent
\textbf{Secondly, we need to find the optimal batch size and momentum parameter}\\
\\Now we want to find the optimal batch size and momentum parameter by looking at a seaborn plot with the momentum parameter and batch size at the x- and y-axis (respectively). To be able to create such a plot, we need to  initialize some parameters (again). Now, we have optimized the architecture of the neural network (by last interpretations), and will be using those as some of the initial values. So, the parameters we now have are:

\begin{lstlisting}[language = Python]
------------------
node_list = [20]*1
n_epochs = 300
lmbda = 0.0001
eta = 0.001
------------------
\end{lstlisting}
If we now insert those parameter values inside \textbf{test 12} in \textbf{test\_project\_2.py}, then we will get two plots (figure \ref{fig: classification_batchSize_gamma_training} and figure \ref{fig: classification_batch_Size_gamma_testing}). By looking at the plots, it seems like the best parameters is to have a gamma value of 0.9, and it seems like it doesn't depend that much on the batch size, so I will in this case go with 14. It can often be a great risk of choosing such a high momentum parameter, gamma, since it can easily fail to converge. Since the model predicts best with such values, we will go for them anyways. 

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test12_M_26_gamma_1.0_lmbda_0.0001_eta_0.001_hidact_sigmoid_outact_sigmoid_training_3.png}
    \captionof{figure}{Data: training, accuracy of different batch size and gamma values}
    \label{fig: classification_batchSize_gamma_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test12_M_26_gamma_1.0_lmbda_0.0001_eta_0.001_hidact_sigmoid_outact_sigmoid_test_3.png}
    \captionof{figure}{Data: testing, accuracy of different batch size and gamma values}
    \label{fig: classification_batch_Size_gamma_testing}
\end{figure}


\\\noindent
\textbf{Last, we need to find the optimal hyperparameter lambda and learning rate}\\
\\Now, we want to find the optimal hyperparameter lambda and learning rate by looking at a seaborn plot. Now, with the parameters we found for the batch size, momentum (in the SGD) and the architecture of the neural network, we are able to find the optimal lambda and eta values. We will try to find those parameters that will get the highest accuracy score. We are setting the parameters in the following way:
\begin{lstlisting}[language = Python]
------------------
node_list = [20]*1
n_epochs =  200
batch_size = 14
gamma = 0.9
------------------
\end{lstlisting}
If we now insert those parameter values into \textbf{test 6} in \textbf{test\_project\_2.py}, we will get two plots (figure \ref{fig: classification_lmbda_eta_training} and figure \ref{fig: classification_lmbda_eta_testing}). 


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test6_nepochs_200_M_14_gamma_0.9_features_2_hiddennodes_20_hiddenlayer_1_actOUT_sigmoid_training.png}
    \captionof{figure}{data: training, accuracy of different lambda and eta values}
    \label{fig: classification_lmbda_eta_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test6_nepochs_200_M_14_gamma_0.9_features_2_hiddennodes_20_hiddenlayer_1_actOUT_sigmoid_test.png}
    \captionof{figure}{data: testing, accuracy of different lambda and eta values}
    \label{fig: classification_lmbda_eta_testing}
\end{figure}
\noindent
Figure \ref{fig: classification_lmbda_eta_training} and \ref{fig: classification_lmbda_eta_testing} tells us that a learning rate up towards 1e-3 is a good value, but the risk of getting exploding gradients will be higher with a greater learning rate. Therefore, we found it necessary to restrict the eta values that we are looping over (more than before). The learning rate is so sensitive because the momentum parameter (gamma) is at 0.9, which means that is will use 90\% of the last gradient value into the new iteration. The figures shows us that it doesn't depend that much on the parameter lambda, most  likely because we have chosen a simple architecture which is not likely to lead to an overfit. Although, we can still see some trends for lambdas work as an \textit{anti-overfitting} parameter.

\paragraph{Different activation functions}
%Activation functions: test all three, choose best
Since we are dealing with a binary classification case, the sigmoid function is obviously the best choice for the output layer. For the hidden layers, we study how the sigmoid function compares to the RELU and leaky RELU. First, we plot the accuracy score for the training data over iterations with the sigmoid activation layers. The following plots (fig. \ref{fig: classification_accuracy_over_time_sigmoid}, \ref{fig: classification_accuracy_over_time_RELU}, \ref{fig: classification_accuracy_over_time_leaky}) are achieved by running \textbf{Test 5} in the \textbf{test\_project\_2.py} - file, with the same parameters that we have been chosen as described in the previous subsection, that is:
\begin{lstlisting}[language = Python]
--------------------------------------------------------
number_of_epohcs = 200
batch_size = 14
gamma = 0.9
--------------------------------------------------------
\end{lstlisting}

and changing the hidden activation function between (respectively):

\begin{lstlisting}[language = Python]
--------------------------------------------------------
FFNN.set_activation_function_hidden_layers('sigmoid')
FFNN.set_activation_function_hidden_layers('RELU')
FFNN.set_activation_function_hidden_layers('Leaky_RELU')
--------------------------------------------------------
\end{lstlisting}



\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{sigma.png}
    \captionof{figure}{Accuracy of training data over iterations with the sigmoid function for hidden layers}
    \label{fig: classification_accuracy_over_time_sigmoid}
\end{figure}

\noindent
We create the same plot for the RELU function for the hidden layers. We have changed the $\gamma$ parameter from 0.9 to 0.5 as this gives a better result:

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{accuracy_of_last_training_0.5_0.0008.png}
    \captionof{figure}{Accuracy of training data over iterations with the RELU function for hidden layers}
    \label{fig: classification_accuracy_over_time_RELU}
\end{figure}

\noindent
Finally we create the same plot for the leaky RELU with the same parameters as for the RELU:

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{stinky.png}
    \captionof{figure}{Accuracy of training data over iterations with the leaky RELU function for hidden layers}
    \label{fig: classification_accuracy_over_time_leaky}
\end{figure}

\noindent
We observe that all the activation functions for the hidden layers give an accuracy score of above 90$\%$, yet the leaky RELU function seems to coverge to an accurate model in fewer iteration than both the sigmoid and the normal RELU, hence it is the best choice for this model. 


\subsubsection{Logistic regression}
Now, we want to look at how well the logistic regression works for predicting the same data as we have studied in the last section. We want to find the optimal values for the learning rate and the hyperparameter lambda. We will do that by compare the different accuracy score the model will give us with the different parameters. To do so, we need to start with some specified parameters, which are:

\begin{lstlisting}[language = Python]
---------------
n_epochs = 40
batch_size = 14
gamma = 0.8
---------------
\end{lstlisting}
\noindent
With these values, we make a heat map for different $\eta$ and $\lambda$'s to find the optimal choice:

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test7_nepochs_40_M_14_gamma_0.8_features_2_test.png}
    \captionof{figure}{Accuracy of different $\eta$ and $\lambda$ values for the testing data}
    \label{fig: classification_accuracy_logistic}
\end{figure}
\noindent
In figure \ref{fig: classification_accuracy_logistic}, we observe that the optimal parameters are $\eta = 0.1, \lambda = 10^{-3}$. These are the parameters we will be using when comparing with the neural network. 

\subsubsection{Comparison: Neural network and logistic regression}
The values we are gonna use for the comparison between the different methods for modelling the classification case are the optimal ones from the last two sections, which are:

\begin{lstlisting}[language = Python]
----------------------------
# Values for neural network
n_epochs = 4
batch_size = 14
gamma = 0.5
eta = 0.001
lmbda = 1e-5
FFNN.set_cost_function(logistic_cost_NN)
FFNN.set_activation_function_hidden_layers('RELU')
FFNN.set_activation_function_output_layer('sigmoid')

# Values for logistic regression
n_epochs = 3
cost_function = cost_logistic_regression
batch_size = 14
gamma = 0.8
eta = 0.1
lmbda = 1e-3
----------------------------
\end{lstlisting}

\noindent
The results is as follows:
\begin{lstlisting}[language = Python]
-------------------------------------------------------------
>> RUNNING TEST 8:
>>> Results of classification problem with Neural Network <<<
ACCURACY_train =  0.9108
ACCURACY_test =  0.9161
TIME SPENT:  0.069

>>> Results of logistic regression, with SGD <<<
ACCURACY_train =>  0.9178
ACCURACY_test =>  0.9161
TIME SPENT:  0.134
-------------------------------------------------------------
\end{lstlisting}
\noindent
We have done some testing to find two great models with respect to the number of epochs, because we wanted to find models that predict well and train quickly. We can see that for similar accuracy, in both the training- and test data, the time spent training the neural network is half of the logistic regression case. 

\section{Conclusion}

We started by looking at the Stochastic Gradient Descent algorithm (SGD). The method is an alternative to a matrix inversion, since we often use matrix inversion as a calculation for getting the analytical solution to the problem. The matrix inversion is time consuming, and therefore we introduced the gradient descent algorithm to train the model. The stochastic in the gradient descent is also a further improvement to the algorithm, where we consider a smaller amount of data to train the model with, which would make the process even faster. Now, we have a method for tune some the parameters of the model, but the hyperparameters sent in to the training must be optimized in some way for the model to be a good one. This is the next problem we are up against. \\

\noindent
One of the first thing we found out was that the learning rate and gamma had a huge impact on the results we were gonna achieve. A too high learning rate would often result in a bad prediction or divergence with respect to the tuned parameters, and a too little learning rate would converge really slow (if it even converged to the \textit{right} solution). The combination between the learning rate and the momentum parameter was critical to success, where the momentum helped a smaller learning rate to converge faster. Secondly, we found some \textit{problem} with the algorithm, where the prediction, according to the model, was nearly a constant for all input (local minimum of the cost function). This could be fixed in most cases by refreshing the parameters we wanted to tune, but in the later testing we would refer to those occurrences as \textit{unstable}, where the unstable part was referring to the hyperparameters sent into the training. Finally, we figured out that in most cases the number of iterations was highly correlated with the prediction we got. The higher number of epochs, iterations, the greater model. \\

\noindent
The first thing we noticed when testing our neural network code was that for it to work properly, there are a lot of parameters to be tuned, everything from architecture, learning rate, hyper-parameters to activation functions. We have learned that different data requires different parameters. We have also learned that it is not always given a priori how a change in a parameter affects the result, and this information bust be obtained through tests. We have also realized how small changes in parameters may cause the gradient descent to diverge, and one must be especially careful when choosing the learning rate, as mentioned above. We have also realized that the quality of the neural network may also be limited by the accessible computing power. Training of large architectures requires large computations, and this may be limited if one is using for instance a laptop. Different activation functions also give different results, and some can more easily cause vanishing or exploding gradients than others. \\


\noindent
When comparing our neural network to the linear regression code from project 1, we noticed that the neural network was far slower in achieving about the same results as the linear regression. From this we conclude that when there exist machine learning methods that are less complex than the neural network which also requires less computing time, it might be a better option. Let it be noted that for the linear regression case, we only used two input parameters (since we where looking at the Franke Function data set from project 1), and had we used more, the linear regression code may nor have been as fast anymore. We also note that we could have increased the complexity of the neural network by changing it's architecture, but our code already required heavy computation, and we did not find it necessary to run tests that would have taken even longer time. \\

\noindent
When comparing the neural network to the logistic regression in the classification case, both methods achieved a high accuracy score, but the neural network did it faster. In this case, both methods required gradient descent, and the neural network trained faster than the logistic regression model. We only studied one data set, so it is hard to tell if the neural network would have been faster for another set of data. \\

\noindent
Our time with this project was limited, so it was necessary to restrict the project in some way. Further analysis we would probably have done would be to look at more types of data set. In this case, we have only looked at a binary classification problem (the target value in the classification case is just 0 or 1). The input data, in both the regression- and classification case, had just two input features, since we restricted the data set as explained in section \ref{sec_classification_data}. With new data sets, we would maybe see some new insights and get other results from our analysis than we got in this project. The last thing we would looked into is various number of hidden nodes in the different hidden layers and see if there is some more insights there. The neural network class is ready for handling such analysis, but we did not found it necessary in this project. 


\section{Bibliography}
% TODO: delete
\section{Github repository} \label{sec_github_repository}
\href{https://github.com/oystehbr/FYS-STK4155/tree/main/project2}{Github REPO - project 2}

\end{document}