\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{esint}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{ dsfont }
\usepackage{hyperref}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[font={small,it}]{caption}
\usepackage{caption}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage[euler]{textgreek}
\graphicspath{{./plots/}}
\usepackage{biblatex}
\addbibresource{reff.bib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[font=small,labelfont=bf]{caption}
\setcounter{secnumdepth}{5}

\hypersetup{
    colorlinks=true,
    linkcolor=blue
}

\title{Project 2 FYS-STK4155}
\author{Sigurd Holmsen, Øystein Høistad Bruce}
\date{November 2021}

\begin{document}

\maketitle

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{CASE.NM.BILDE.jpg}
\end{figure}

\newpage

\tableofcontents
\newpage

\section{Abstract}

An abstract where you give the main summary of your work

\section{Introduction}
% TODO: Below need to be included in some way
We have introduced the data in the sections \ref{sec_regression_data} and \ref{sec_classification_data} for the regression- and classification case (respectively).
%Explain neural network, modelled after neurons in biology, motivate!


An introduction where you explain the aims and rationale for the physics case and what you have done. At the end of the introduction you should give a brief summary of the structure of the report

\section{Method}
Here, we explain the functions we have programmed and some concepts behind them. The functions take in input data $X$ and output data $y$, so data must be generated or extracted from somewhere before the models can be used. 
%TODO: explain assumptions for all models and data 
\subsection{Scaling of data}

\subsection{Gradient Descent}
The goal of gradient descent is to find parameters that minimizes a cost function. We have written a gradient descent function that takes in initial values of the parameters that will be tuned, a cost function, the learning rate $\eta$, a hyper-parameter $\lambda$ and the number of iterations, as well as input and output data $X, y$. The learning rate $\eta$ decides the length of each step in the search direction. In this method, we have used the python package autograd for finding the gradients of the cost function. We then use the gradients and the learning rate to converge towards the minimum of the cost function, after which the function returns the updated/ tuned parameters that's defining the model. The algorithm we used is explained in the next subsection. 

\subsubsection{Momentum}
We have used a momentum based gradient descent, which means the length of each step in the gradient descent depends on the size of the previous gradient. The point is to do longer steps when gradients are steeper, and smaller steps when the gradients are flatter. In this way, the gradient descent adjusts to the "terrain" of the cost function. The algorithm can be expressed as follows: 

\begin{align}
    v_t &= \gamma v_{t-1} + \eta_t \frac{d C}{d \theta} \\
    \theta_{t+1} &= \theta_t - v_t
\end{align}

where the parameter $\gamma \in [0, 1]$ is the amount of momentum we want to use (no momentum is 0). $v_t$ is the amount of change in the tuning parameter at time $t$. $\theta$ is the parameter we want to tune, and $\frac{d C}{d \theta}$ is the change in cost with respect to $\theta$.

\subsubsection{Stochastic}
We have made our gradient descent method allowing stochastic, meaning we do not necessarily compute the gradients for the entire data set $X, y$, but for a small, randomly chosen batch $X_k, y_k$. If we let the size of the mini-batches to be $M$, then we will have $m$ mini-batches, where $m=\frac{\text{number of data}}{M}$ (rounded down to nearest integer). We then select a random integer $k\in [0, 1, ... (m-1)]$, and select the k-th mini-batch:

\begin{align*}
    \text{}
    X_k &= X[k \cdot M:(k+1) \cdot M]\\
    y_k &= y[k \cdot M:(k+1) \cdot M]
\end{align*}
In this way, we have produced a randomly chosen mini-batch from $X, y$, and computed the gradient $\frac{\text{number of data}}{M}$ times. We also decide the number of epochs, that is how many times we will repeat this process. The goal of the stochastic gradient descent is to save computational power and time, as we compute fewer gradients. 

\subsection{Neural Network}
\subsubsection{Architecture}
We have made a neural network class. Regarding the network architecture, the class takes the following parameters: number of input nodes, number of output nodes and a list of hidden layers with corresponding nodes. Each element of the list of hidden layers is an integer representing the number of nodes of the related layer. This allows us to set up the network architecture however we would like, where number of nodes per layer can vary. For more information on how to initiate a class, please look at the README file. 
\\ \\
Then we initialize the weights using the normal distribution and the biases as small float numbers. The class also has parameters for the activation function for the hidden layers and for the output layer, where the sigmoid function is the default activation function. Then, the user can choose the values for the stochastic gradient descent. That includes the learning rate $\eta$, the regularization parameter $\lambda$, batch size and number of epochs in the SGD, and the hyper-parameter $\gamma$.
\\ \\
The program does not scale data, so this must be done beforehand if needed. For more information on how the code works, look at the comments and doc-strings in the code. 

\subsubsection{Regularization parameters}
Our network takes a regularization parameter $\lambda$ which adds a term to the cost function:
$\lambda  \left\lVert w \right\rVert^2_2$. This term is added in attempt to avoid an overfit of the model. This is implemented as a parameter in the Neural Network class as explained above. 
% TODO: vanishing gradients
% TODO: exploding gradients

\subsubsection{Hyperparameter}
The network has also a method for choosing the hyperparameter $\gamma$, which decides the amount of momentum applied to the gradient descent. This may help finding a minimum quicker. This is implemented as a parameter in the Neural Network class as explained above. % TODO: maybe refer to github instead?

\subsubsection{Cost Function}
When a neural network is initialized, one has to decide which cost function to use, as this will influence the gradient descent, or more specifically the back propagation algorithm. The cost function must be given in a method inside the class. It is important that the cost function can be interchanged, as the linear regression and classification cases use different cost functions. 

\subsubsection{Feed Forward}
The feed-forward function takes a set of data and predicts the output given the model weights and biases. The data should be a matrix of dimension $(\#data, \#features)$. This means we are approximating several output values simultaneously. Each node is connected to all the nodes in the next layer, and every node has its own bias. After each node has received values from the previous nodes, multiplied with the weight matrix and added with the bias, it it sent through the hidden layer activation function. This value is sent to the nodes in the next layer, and this process is repeated until the output node(s) are reached. The output node(s) may have a different activation function than the hidden layers, or it may not have an activation function at all. 

\subsubsection{Activation functions}
The activation functions are used in the feed forward part underneath, the different activation function we are looking at in this project are:\\

\\\noindent
\textbf{Sigmoid}
\begin{equation}
    f(z) = \frac{1}{1+\exp{-z}}
\end{equation}

\\\noindent
\textbf{Rectified Linear Unit (RELU)}
\begin{equation}
    f(z) = 
        \begin{cases}
            \, z, & z > 0\\
            \, 0 & \text{otherwise}
        \end{cases}
\end{equation}

\\\noindent
\textbf{Leaky Rectified Linear Unit (Leaky RELU)}
\begin{equation}
    f(z) = 
        \begin{cases}
            \, z, & z > 0\\
            \, -\alpha z & \text{otherwise}
        \end{cases}
\end{equation}

for some (small) $\alpha > 0$\\

\subsubsection{Back Propagation}
% TODO: do we need to explain the SGD again?
To tune our weight and bias parameters, we use a stochastic gradient descent (SGD). That means we randomly pick a batch from $X$, and the corresponding output values from $y$, and compute the gradients of the parameters from these. We use a momentum based SGD. To find the gradients for the weights and biases, we perform the back propagation algorithm. First, we choose a cost-function, for instance a variant of the mean square error, as a measurement of the quality of our model:

\begin{equation}
    MSE = \frac{1}{2n} \sum_i (y_i - \tilde{y}_i)^{2}
\end{equation}
Then we are interested in finding the gradients for a given layer $l$:

\begin{equation}
    \frac{\delta C}{\delta w^l_{ij}}, \, \frac{\delta C}{\delta b^l_j}
\end{equation}
According to the back propagation algorithm, this can be written as:

\begin{align}
    \frac{\delta C}{\delta w^l_{ij}} = \delta_j^l a_k^{l-1}, \, \frac{\delta C}{\delta b^l_j} = \delta_j^l
\end{align}

where

\begin{equation}
    \delta_j^l = \sum_k \delta_k^{l+1} w_{kj}^{l+1} f'(z^l_j)
\end{equation}
Here, $f$ is the activation function for the given layer. We can compute this if we know the value of $\delta^{l+1}$. We can compute, the last layer delta in such way:

\begin{equation}
    \delta^L = \frac{1}{2n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\end{equation}
So, we are able to find all the $\delta$ values starting backwards. After we have found all the gradients for both the weights and biases, we use this for the momentum based SGD in the same way as described in the section above. When the SGD has iterated through all the given epochs, the training of the model is complete. 

\subsubsection{Regression case}
% Scaling data may be smart to not set exploding gradients after first iteration (with random weights)
For the regression case, we will not need an activation function for the output layer. This is because we may want to fit any number from $-\infty$ to $\infty$, and then we do not want to put a boundary on the value of the output. We also use the MSE as the cost function, which is accounted for in the back propagation algorithm.
% TODO: the MSE or a variant of it

\subsubsection{Classification case}
% Scaling the data 
% Scaling data may be smart to not set exploding gradients after first iteration (with random weights)
For the classification case, we want the output to be interpreted as probabilities. Hence we want to limit the output between 0 and 1. Since we are studying a binary classification problem, we use the sigmoid function as the output layer activation function. We have used the log-loss cost function when deriving the back propagation algorithm:

\begin{equation}
    C(W) = - \sum_{i=0}^n y_i log(\hat{y}_i) + (1-y_i) log(1 - \hat{y}_i)
\end{equation}

When using the neural network, we have used cancer data from the Wisconsin studies, and have utilized principal component analysis to rule out the two most important features from the data set. Principal component analysis looks at the unit vectors (representing the features) with the greatest eigenvalues, and we then only send the chosen data into our neural network for the training. 


\subsection{Logistic Regression}
When performing the logistic regression, we have used the following cost function: 

\begin{equation}
    \prod_{i=0}^n \, p(x_i=1 | \beta)^{y_i} p(x_i=0|\beta)^{1-y_i}
\end{equation}

Where
\begin{equation}
    p(x=1|\beta) = \frac{e^{\beta_0 + \beta_1x_1 + ... + \beta_nx_n}}{1 + e^{\beta_0 + \beta_1x_1 + ... + \beta_nx_n}}
\end{equation}
\noindent
Then, we perform the SGD as before, where we use the autograd package to find the gradients of the cost function. 


\newpage
\section{Results and discussion}
\subsection{Introduction to the results}
In this section, we will refer to some tests we have made in the \textbf{test\_project\_2.py} which will be found in the \textbf{project2} folder inside the github repository, attached TODO. To have a great experience with the testing file, we recommend you to read the \textbf{README.md} file inside the \textbf{project2} folder. We will provide the correct parameters for doing the tests while we present the results.

\subsubsection{General comments on the results}
Here is a list of things to be aware of according to the results we are presenting:
\begin{enumerate}
    \item When we are comparing two parameters, we looking at the R2-score or accuracy score to the regression- and classification problem (respectively). Among some great results there will be some \textit{wierd} results also (even with a small change of a parameter value). This is often some case where the model are going to predict the same target value for every kind of input value. Such cases can maybe be some \textit{local} minimum of the cost-function. We have not \textit{fixed} those occurrences, but we know about the problem and after some testing it would help to re-initializing the weights and biases. TODO: finish this
    \item In our analysis, we had to create up some starting values for the parameters we wanted to tune. Whenever we got out some results from a previous testing, we added the greatest parameters to the new testing. TODO (maybe): it would probably be great to go over more starting values and do the fully process on those (and then compare) - (kan juge her), we have done it and the starting values is chosen in such way. 
    \item The number of epochs we have chosen in the analysis isn't optimized in some way. Greater iterations will, very often, get to a better model, as a result we have chosen to go with some \textit{friendly} number of epochs to save computing expenses. 
\end{enumerate}
% TODO: Explain usage of n_epochs -> computing power -> more iterations -> better results. n_epochs = 300 (this i picked by have many iterations, but also computational (often)
% TODO: Maybe show of some plot 

\subsubsection{Introducing the data - regression case} \label{sec_regression_data}
% TODO: mention that when we are looking at Franke Function data, then we use 100 datapoints -> and n_epochs = 300 (this i picked by have many iterations, but also computational friendly). noise to the FrankeFunction is 0.01
% TODO: explain that the regression cases in the results, then we are using the Franke Function and classification case then we are using breasts -> 2 components (argue why we are using 2, maybe show PCA-results. variance_explained_ratio??)
For the testing of the stochastic gradient descent and the regression case of the neural network, we have generated data from the Franke function similarly as in project 1. That means we generate $x, y$ -pairs, both randomly selected between 0 and 1, and send them into the Franke function. A noise term is then added, which is a normally distributed value with a noise multiplier parameter $noise$. \\
\\In our analysis, we have looked at a dataset consists of 100 datapoints, and a noise of 0.01. 

\subsubsection{Introducing the data - classification case} \label{sec_classification_data}
For the classification case, we have used the Wisconsin cancer data found in the Sci-kit learn package $sklearn.datasets$. The data consist of 569 observations and 30 features. If you have much data, then you use all of it to create a best possible model. We are no exception, and we will consider all the observations in our analysis.\\
\\\noindent
We only need to consider two features from the data set, and that is because we are able to predict very well with just those two. We are choosing them optimally by looking up the two most important features according to the principal component analysis (PCA), where the two most important features are having a total \textit{explained variance ratio} above $0.99$ (which is really high). 

We scale the data by the maximum of each input feature, so that the largest input value will be 1.

% TODO: say amount of data
% TODO: 2 components (argue why we are using 2, maybe show PCA-results. variance_explained_ratio??)

\subsubsection{Initializing the weights and biases}
We are initializing the weights according to a standard normal distribution and all the biases are valued 0.01 before it starts to optimize these parameters. We are initializing the biases at 0.01 since it ensures that all neurons have some output which can be backpropagated in the first training cycle.\\

\noindent
\\We have looked at the possibility of refreshing the weights and biases when the predictions goes horrible, often when the model is predicting the same targets for all kind of input, but were told under the project that this wasn't a great way to fix the problem. Those occasions would we refer to the different parameters as unstable, and we should not handle this problem inside the code. 
% TODO: How would you initialize the biases
 

\subsection{Results of Stochastic Gradient Descent}
\subsubsection{OLS regression}

For the OLS regression, we have compared the R2-score for different learning rates and number of minibatches. We want to check how far we are from the analytical solution from project number 1. The following values are being set (and are looking at the Franke Function data, as mentioned above):
\begin{lstlisting}[language = Python]
----------------------------------------------------------
number_of_epochs = 200
degree = 5
gamma = 0.7
run_main_OLS = True
list_number_of_minibatches = [1, 5, 8, 10, 16, 20, 40, 80]
----------------------------------------------------------
\end{lstlisting}
In this case, we want to look at the number of iterations in the SGD-algorithm to be the same for different number of minibatches, such that the values will not be affected that much from the big increasing of iteration by increasing the number of minibatches. 
With these values, we can turn in to the \textbf{test\_project\_2.py} file, and go into \textbf{test 1}. When we run the code, we will be provided with these figures (fig. \ref{fig: regression_OLS_main_batch_eta_training} and \ref{fig: regression_OLS_main_batch_eta_test}).

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{test1_OLS_gamma_0.7_training_11.png}
    \captionof{figure}{Data: training, R2-score of combination of number of minibatches and the learning rat}
    \label{fig: regression_OLS_main_batch_eta_training}
\end{figure}

\noindent
We observe that the optimal learning rate is around $0.1$ and $0.01$ for this case, as this gives the highest R2-score. As a result of that, it will be the closest one to the analytical solution. When we run \textbf{test 1}, we will also be informed, with a print in the terminal, that the R2-score for training and testing is around 0.982 and 0.964 (respectively). Which is not that far away from the results we are getting in these numerical approaches, but with a greater number of iteration we would probably get even closer (as we will see in its own analysis).

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{test1_OLS_gamma_0.7_test_11.png}
    \captionof{figure}{Data: test, R2-score of combination of number of minibatches and the learning rate}
    \label{fig: regression_OLS_main_batch_eta_test}
\end{figure}

\noindent\\
If the mini-batch size is too small (equivalent that the number of minibatches is to high), there may be too much randomness in the computed gradients to converge quickly, and if the size is too large, the problem of computing time arises, which defeats the purpose of the stochastic gradient descent. In our case, we do not see that much difference in the R2-score for different batch sizes (TODO: why). Hence, since the mini batch size do not affect the R2-score it would be a great idea to choose the smallest batch size faster computing time. \\

\noindent
\\Now, we want to show that higher $number\_of\_epochs$-value leads to better results (if the solution seems to converge). The following two figures (fig. \ref{fig: regression_OLS_main_batch_eta_test_10} and \ref{fig: regression_OLS_main_batch_eta_test_1}), showing only the data set for testing, are showing that the statement is true. In these figures, we are looking at the same problem as above - but with these values (respectively):

\begin{lstlisting}[language = Python]
--------------------------------
number_of_epochs = 10
number_of_epochs = 1
--------------------------------
\end{lstlisting}
every other value is sat the same as in the previous testing, so we are only interested in looking at the results with respect to the $number\_of\_epochs$. Again, we are looking at \textbf{test 1} in the \textbf{test\_project\_2.py} file. 

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{test1_OLS_gamma_0.7_epochs_10_test_12.png}
    \captionof{figure}{Same as figure \ref{fig: regression_OLS_main_batch_eta_test}, but $number\_of\_epochs = 10$}
    \label{fig: regression_OLS_main_batch_eta_test_10}
\end{figure}

\noindent
By looking at figure \ref{fig: regression_OLS_main_batch_eta_test}, \ref{fig: regression_OLS_main_batch_eta_test_10} and \ref{fig: regression_OLS_main_batch_eta_test_1} we can confirm that higher number of iterations in the SGD-algorithm will lead to a better result. Then a question arises, why do we not increase the number of epochs to infinity? The answer is simple, the computing time will be increased as the number of epochs increases. Therefore, we need to restrict the number of epochs for us to be able to move forward with the analysis. So, in later analysis we have not optimized the number of epochs in some way, since it is not optimizable. Therefore, we have often picked the parameter so the runtime is not too large. 

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{test1_OLS_gamma_0.7_epochs_1_test_12.png}
    \captionof{figure}{Same as figure \ref{fig: regression_OLS_main_batch_eta_test}, but $number\_of\_epochs = 1$}
    \label{fig: regression_OLS_main_batch_eta_test_1}
\end{figure}

\paragraph{Scaling the learning rate}
\\\noindent
We want to study an algorithm for scaling the learning rate. We have tried the following algorithm:

\begin{equation}
    f(t) = \frac{5}{t + 50}
\end{equation}

\noindent
This algorithm scales the learning rate to be smaller over time. We compare the results with and without the algorithm, where we run five tests both with and without the scaling algorithm. You can find this test by navigate in the \textbf{test\_project\_2.py} file to \textbf{test 13}. We use these values for testing:

\begin{lstlisting}[language = Python]
-----------------
n_epochs = 200
gamma = 0
eta = 0.1
batch_size = 10
-----------------
\end{lstlisting}
\noindent
This would give out a result in this form:

\newpage
\begin{lstlisting}[language = Python]
----------------------------------------
With Scaling learning rate algorithm 
Test 1: MSE: 0.189056
Test 2: MSE: 0.189427
Test 3: MSE: 0.189085
Test 4: MSE: 0.188920
Test 5: MSE: 0.188831
Without Scaling learning rate algorithm
Test 1: MSE: 0.184023
Test 2: MSE: 0.183986
Test 3: MSE: 0.184069
Test 4: MSE: 0.183949
Test 5: MSE: 0.184023
----------------------------------------
\end{lstlisting}

\noindent
Since we do not observed an improved MSE, we do not use the scaling algorithm for the rest of the project as this saves time and may also lead to better results. We already have a momentum based SGD, so perhaps the scaling algorithm is somewhat redundant. 

\subsubsection{Ridge regression}
We make a graph comparing the R2-score of our model as a function of learning rate and hyper-parameter $\lambda$ for Ridge regression.\\

\\\noindent
We observe that the results are sensitive to the learning rate $\eta$, based on figure \ref{fig: regression_RIDGE_main_lmbda_eta_test}. A too large learning rate may cause the gradient descent to miss the global minimum we are trying to find, while a too small learning rate may not converge quickly enough to the minimum. When comparing different learning rates to each other, we have used static learning rates to better see the difference in results. We also see that a too high hyper-parameter $\lambda$ will unnecessarily punish good fits while too low $\lambda$ will fail to punish overfitting. The optimal choice seems to be $\lambda = 10^{-5}, \eta = 1$. Note that when attempting to use higher learning rates, the program crashed as our parameters diverged. We can clearly see that the accuracy of the model is more sensitive to the learning rate $\eta$ than the hyper-parameter $\lambda$. This indicates that we should be careful when choosing the learning rate, as it may greatly change the resulting model.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{test1_nepochs_200_M_100_gamma_0_numdata_500_noOfMinibatches_10_degree_4_test.png}
    \captionof{figure}{Plot of the R2-score of the model as function of learning rate $\eta$ and hyper-parameter $\lambda$. We have used a degree of 4 for the polynomials, $batch\_size = 10$, $number\_of\_epochs = 200$}
    \label{fig: regression_RIDGE_main_lmbda_eta_test}
\end{figure}

\noindent
Then, we find the best mini-batch size. We run a test comparing the MSE for different sizes:

\begin{lstlisting}[language = Python]
--------------------------------------------
MSE: 0.232627 (NUMERICAL (batch size = 1))
MSE: 0.232035 (NUMERICAL (batch size = 5))
MSE: 0.231755 (NUMERICAL (batch size = 10))
MSE: 0.231771 (NUMERICAL (batch size = 30))
MSE: 0.231771 (NUMERICAL (batch size = 60))
MSE: 0.231758 (NUMERICAL (batch size = 100))
--------------------------------------------
\end{lstlisting}

\noindent
We notice that a batch size of 10 gives the lowest MSE, hence this is the optimal choice for the ridge regression. Since there are 80 data points for training (100 in total), a batch size of 10 gives out 8 mini batches. 

\newpage
\subsection{Testing the neural network (regression case)}
% TODO: order: architecture, batch_size and gamma, lmbda and learning rate
%Mostly problem b
%Analysis of the regularization parameters (task b)
%Seaborn
We will use our neural network to predict the Franke function as in project 1. How we extracted our data is described in \ref{sec_regression_data}. To make the model work as well as possible, we have to choose the correct parameters. With the Franke function data set, we will not set any activation function for the output layer, since we have not scaled the data in this case. That is because our neural network is able to handle all values, not just values between 0 and 1. We have chosen to start by studying with the Sigmoid as an activation function for the hidden layers. \\

%seaborn - comment too high or too low parameters
\\\noindent
\textbf{First, we need to find the optimal architecture of the neural network}\\
\\We are finding the optimal architecture by looking at a seaborn plot with the number of layers and nodes at the x- and y-axis (respectively). To be able to create such a plot, we need to initialize some parameters. We started by setting the parameters in the following way:

\begin{lstlisting}[language = Python]
-----------------
n_epochs = 300
batch_size = 10
gamma = 0
lmbda = 0.0001
eta = 0.001
-----------------
\end{lstlisting}
With these values, we received the following plots as shown in figure \ref{fig: regression_architecture_training} and figure \ref{fig: regression_architecture_testing}, by using \textbf{test 9} in \textbf{test\_project\_2.py}. The plot tells us how we should build up our neural network by picking some architecture, a combination of the amount of nodes and layers, that gives us the highest R2-score. \\

\\\noindent
By looking at the figures \ref{fig: regression_architecture_training} and \ref{fig: regression_architecture_testing}, we can observe that the amount of nodes need to be (much) greater than the number of hidden layers for the neural network to be good as a model. We can also see that among the cells with the highest R2-score, there will be some "bad" ones also. Those cells (with R2-score around 0), have gone to a local minimum - where the model is predicting the same target values for all kind of input data. It is possible to avoid many of such occasions, by refreshing the weights and biases (start the algorithm over again), but it looks like we shouldn't do so (TODO). We tried to find a region where the architecture was kind of stable and worked good as a model, those values were 40 hidden nodes and 3 hidden layers. So, from now on we are going to evaluate the remaining parameters given this architecture.\\
\\\noindent
A too high complexity of the model will be more computationally expensive, and may also result in an overfit, while a too simple model seems to give an underfit.

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test9_M_10_gamma_0_lmbda_0.0001_eta_0.001_training_1.png}
    \captionof{figure}{Data: training, R2-score of different architecture of the neural network}
    \label{fig: regression_architecture_training}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test9_M_10_gamma_0_lmbda_0.0001_eta_0.001_test_1.png}
    \captionof{figure}{Data: testing, R2-score of different architecture of the neural network}
    \label{fig: regression_architecture_testing}
\end{figure}


\\\noindent
\textbf{Secondly, we need to find the optimal batch size and momentum parameter}\\
\\Now we want to find the optimal batch size and momentum parameter by looking at a seaborn plot with the momentum parameter and batch size at the x- and y-axis (respectively). To be able to create such a plot, we need to  initialize some parameters (again). Now, we have optimized the architecture of the neural network (by last interpretations), and will be using those as initial values. So, the parameters we now set are:

\begin{lstlisting}[language = Python]
------------------
node_list = [40]*3
n_epochs = 300
lmbda = 0.0001
eta = 0.001
------------------
\end{lstlisting}
If we now insert those parameter values inside \textbf{test 10} in \textbf{test\_project\_2.py}, then we will get two plots (figure \ref{fig: regression_batchSize_gamma_training} and figure \ref{fig: regression_batch_Size_gamma_testing}). By looking at the plots, it seems like the most "stable" values (the models that are not predicting the same output) lies in the middle of the plot. We can see the same trend in the R2-score for both training and testing data. So, we have chosen to go further with a batch size of 16, and a gamma value of 0.4


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test10_lmbda_0.0001_eta_0.001_hiddennodes_40_hiddenlayer_3_training_1.png}
    \captionof{figure}{Data: training, R2-score of different batch size and gamma values}
    \label{fig: regression_batchSize_gamma_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test10_lmbda_0.0001_eta_0.001_40_hiddenlayer_3_test_1.png}
    \captionof{figure}{Data: testing, R2-score of different batch size and gamma values}
    \label{fig: regression_batch_Size_gamma_testing}
\end{figure}

\\\noindent
\textbf{Last, we need to find the optimal hyperparameter lambda and learning rate}\\
% TODO: didn't finish this!!!!
\\Now we want to find the optimal hyperparameter lambda and learning rate by looking at a seaborn plot. Now, by the parameters we found for the batch size, momentum (in the SGD) and the architecture of the neural network, we are able to use those values for finding the optimal lambda and eta values. We will try to find the parameters, $\eta$ and $\lambda$, with the greatest R2-score. We are setting the parameters in the following way:
\begin{lstlisting}[language = Python]
------------------
node_list = [40]*3
n_epochs = 2000
batch_size = 16
gamma = 0.4
------------------
\end{lstlisting}
If we now insert those parameter values to \textbf{test 4} in \textbf{test\_project\_2.py}, then we will get two plots (figure \ref{fig: regression_lmbda_eta_training} and figure \ref{fig: regression_lmbda_eta_testing}). The figures tells us that the best learning rate, eta, is around 1e-3 and 1e-4 and the most stable lambdas are between 1e-3 and 1e-6. 


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test4_M_16_gamma_0.4_hiddennodes40_hiddenlayers_3_training_1.png}
    \captionof{figure}{data: training, R2-score of different lambda and eta values}
    \label{fig: regression_lmbda_eta_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test4_M_16_gamma_0.4_hiddennodes40_hiddenlayers_3_test_1.png}
    \captionof{figure}{data: training, R2-score of different lambda and eta values}
    \label{fig: regression_lmbda_eta_testing}
\end{figure}

\noindent
We can see at fig. \ref{fig: regression_lmbda_eta_training}, \ref{fig: regression_lmbda_eta_testing} that for a greater lambda value, the model will not be as overfitted as it would be without such a parameter. Also, we observe that with a greater learning rate than 1e-3 we will be in a difficult situation where the model will predict totally wrong, where the tuned parameters (weights, biases) have probably got some exploding- or vanishing gradients (TODO). With a too low learning rate, we observe that in some cases the training have gone too slow to reach some great tuned parameters. This could be fixed by setting up the number of epochs, but this would require more computing power which is something we want to avoid if possible. The optimal parameters in this case are: $\eta = 10^{-3}$, $\lambda = 10^{-4}$, $\gamma = 0.4$, batch size = 14, layers = 4, nodes in hidden layers = 40.


\subsubsection{Activation functions}
So, the fully analysis we have done so far was with the Sigmoid as the activation function for the hidden layer. Now, we are ready for looking at some other activation functions. We have gone into details in the Sigmoid case, and will just briefly comment the optimization of the coming activation functions
% TODO: tell it in the beginning of the section

\paragraph{the RELU}
To study the RELU function, we find all other parameters again. We find the optimal degrees, nodes, $\gamma$, batch size, $\eta$ and $\lambda$ again:

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test9_M_10_gamma_0_lmbda_0.0001_eta_0.001_test_layerNameRELU_1.png}
    \captionof{figure}{Studying the R2-score with the RELU activation function for different layers and number of nodes}
    \label{fig: regression_architecture_RELU_testing}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{pro.png}
    \captionof{figure}{Studying the R2-score with the RELU activation function for different batch sizes and $\gamma$ values}
    \label{fig: regression_batchSize_gamma_RELU_testing}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test4_M_6_gamma_0.6_hiddennodes8_hiddenlayers_4_test_1.png}
    \captionof{figure}{Studying the R2-score with the RELU activation function for different learning rates and $\lambda$}
    \label{fig: regression_lmbda_eta_RELU_testing}
\end{figure}

We note that when the learning rate was higher than $10^{-3}$, the code did not run, presumptuously due to exploding gradients. It appears that the optimal parameters for the RELU activation function are $\eta = 10^{-3}, \lambda = 10^{-5}, \gamma = 0.5$, batch size = 6, number of layers = 4, nodes per layer = 8.

According to fig.\ref{fig: regression_lmbda_eta_RELU_testing}, we achieve an R2-score of 0.87 for the testing data. Compared to the same tests for the sigmoid function, the RELU activation function does not seem to tolerate many hidden layers and many nodes, as this may cause gradients to vanish or explode. We sometimes get Runtime Warnings for underflow from python which indicated that gradients are vanishing, which can be expected of the RELU function as the gradient is zero for certain input values. We get a slightly higher R2-score with the RELU than the sigmoid, meaning RELU might be the better choice for the data we are working with. 


\paragraph{Leaky RELU}

To study the leaky RELU, we find all the optimal parameters using the leaky RELU activation function for the hidden layers:

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test9_M_10_gamma_0_lmbda_0.0001_eta_1e-05_test_layerNameLeaky_RELU_1.png}
    \captionof{figure}{Studying the R2-score with the Leaky RELU activation function for different amount of layers and nodes}
    \label{fig: regression_lmbda_eta_RELU_testing}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test10_lmbda_0.0001_eta_0.0001_8_hiddenlayer_4_hidden_layer_name_Leaky_RELU_test_2.png}
    \captionof{figure}{Studying the R2-score with the Leaky RELU activation function for different $\gamma$ values and batch sizes}
    \label{fig: regression_lmbda_eta_RELU_testing}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test4_M_5_gamma_0.6_hiddennodes8_hiddenlayers_4_name_hidden_Leaky_RELU_test_2.png}
    \captionof{figure}{Studying the R2-score with the Leaky RELU activation function for different learning rates $\eta$ and $\lambda$ values}
    \label{fig: regression_lmbda_eta_RELU_testing}
\end{figure}

We conclude that the following parameters seem optimal for the Leaky RELU activation function: $\eta = 10^{-3}, \lambda = 10^{-5}, \gamma = 0.6$, batch size = 5, number of layers = 4, nodes per layer = 8. For these parameters we achieve an R2-score of 0.82 for the test-data, slightly less than for the RELU activation function.

The results look similar to the ones for the RELU function, but the leaky RELU seems more stable, as the program seems more stable and we do not get Runtime Warnings from python because of disappearing gradients. 

\paragraph{Evaluation}
We observe that all the activation functions achieve roughly the same R2 scores. The computing time is also similar. Meanwhile, one has to be more careful when choosing parameters for the RELU function, as the gradients can more easily vanish which causes a terrible model. 

\subsubsection{Comparison with Linear Regression}
%Observe: the neural network may depend on the inital weights and biases
%Linear regression not good for tuning many variables
We compare out neural network to our code from project 1, i.e. the normal linear regression with an analytical solution. We are still using the data form the Franke function. We compare both R2-scores and the running time of each method. We run the test with the optimal parameters for the sigmoid function as chosen in section 4.3, and the test can be reproduced by running \textbf{test 3} in our test file:

\begin{lstlisting}[language = Python]
----------------------------------------
> Neural Network (time spent: 12.7519s):
** (R2) TRAINING DATA: 0.822
** (R2)  TESTING DATA: 0.7969

> OLS (time spent: 0.0005s):
** (R2) TRAINING DATA: 0.9724
** (R2)  TESTING DATA: 0.9636

> RIDGE (time spent: 0.0003s):
** (R2) TRAINING DATA: 0.9201
** (R2)  TESTING DATA: 0.8661
----------------------------------------
\end{lstlisting}

\noindent
We observe that in this case, our neural network does not only achieve a lower R2-score than both the OLS and Ridge regression methods, it also much slower. The reason for this is probably that we are only using 2 input variables (as in project 1), and the matrix inversion performed in OLS and Ridge regression does not require heavy computation. If we instead had looked at a data set with hundreds of input variables, the neural network would most likely be a better model as it does not suffer as badly from the curse of dimensionality as matrix inversion. 



\subsection{Testing the neural network (classification case)}
We will use our neural network to predict 0 and 1's of the \textit{Wisconsin Breast Cancer} data set. How we extracted our data is described in \ref{sec_classification_data}. In this case, we have data that shall predict values between 0 and 1 and it will be a much greater idea to choose some kind of an activation function for the output layer too (as well for the hidden layers). In the coming analysis, we have chosen to start looking at the activation function Sigmoid for both hidden- and output layer(s). To make a great model, we need to find the greatest hyperparameters. \\

\\\noindent
\textbf{First, we need to find the optimal architecture of the neural network}\\
\\We are finding the optimal architecture by looking at a seaborn plot with the number of layers and nodes at the x- and y-axis (respectively).To make a great model, we need to find the greatest hyperparameters. We will start out search of a great architecture with these parameters. 

\begin{lstlisting}[language = Python]
--------------------------------
n_epochs = 300
batch_size = 10
gamma = 0
lmbda = 0.0001
eta = 0.001
--------------------------------
\end{lstlisting}


\noindent
With these values, we received the following plots as shown in figure \ref{fig: classification_architecture_training} and figure \ref{fig: classification_architecture_testing}, by using \textbf{test 11} in \textbf{test\_project\_2.py}. The plot tells us how we should build up our neural network by picking some architecture, a combination of the amount of nodes and layers, that gives us the highest accuracy score. \\


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test11_M_10_gamma_0_lmbda_0.0001_eta_0.001_hidact_sigmoid_outact_sigmoid_training_6.png}
    \captionof{figure}{Data: training, accuracy of different architecture of the neural network}
    \label{fig: classification_architecture_training}
\end{figure}

\\\noindent
By looking at the figures \ref{fig: classification_architecture_training} and \ref{fig: classification_architecture_testing}, we do observe some similarities with the regression case, that the number of nodes need to be greater than the number of hidden layers for the model to predict well. We will again find some architecture that is kind of stable, in the sense of slightly different parameters will not predict much worse, but also try to find the combination of nodes and layers that will achieve the highest accuracy score. 
\\\noindent

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test11_M_10_gamma_0_lmbda_0.0001_eta_0.001_hidact_sigmoid_outact_sigmoid_test_6.png}
    \captionof{figure}{Data: testing, accuracy of different architecture of the neural network}
    \label{fig: classification_architecture_testing}
\end{figure}
\noindent
Fig. \ref{fig: classification_architecture_training} and \ref{fig: classification_architecture_testing} are telling us that the number of nodes and layers should be about 20 and 1 (respectively). So, we are going to use those values in the further analysis of the other parameters we want to tune. We pick 20 as the number of hidden nodes because the lower amount of nodes, the more \textit{computing friendly it gets}.\\

\\\noindent
\textbf{Secondly, we need to find the optimal batch size and momentum parameter}\\
\\Now we want to find the optimal batch size and momentum parameter by looking at a seaborn plot with the momentum parameter and batch size at the x- and y-axis (respectively). To be able to create such a plot, we need to  initialize some parameters (again). Now, we have optimized the architecture of the neural network (by last interpretations), and will be using those as some of the initial values. So, the parameters we now have are:

\begin{lstlisting}[language = Python]
------------------
node_list = [20]*1
n_epochs = 300
lmbda = 0.0001
eta = 0.001
------------------
\end{lstlisting}
If we now insert those parameter values inside \textbf{test 12} in \textbf{test\_project\_2.py}, then we will get two plots (figure \ref{fig: classification_batchSize_gamma_training} and figure \ref{fig: classification_batch_Size_gamma_testing}). By looking at the plots, it seems like the best parameters is to have a gamma value of 0.9, and it seems like it doesn't depend that much on the batch size, so I will in this case go with 14. It can often be a great risk of choosing such a high momentum parameter, gamma, since it can easily fail to converge. Since the model predicts best with such values, we will go for them anyways. 

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test12_M_26_gamma_1.0_lmbda_0.0001_eta_0.001_hidact_sigmoid_outact_sigmoid_training_3.png}
    \captionof{figure}{Data: training, accuracy of different batch size and gamma values}
    \label{fig: classification_batchSize_gamma_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test12_M_26_gamma_1.0_lmbda_0.0001_eta_0.001_hidact_sigmoid_outact_sigmoid_test_3.png}
    \captionof{figure}{Data: testing, accuracy of different batch size and gamma values}
    \label{fig: classification_batch_Size_gamma_testing}
\end{figure}


\\\noindent
\textbf{Last, we need to find the optimal hyperparameter lambda and learning rate}\\
\\Now, we want to find the optimal hyperparameter lambda and learning rate by looking at a seaborn plot. Now, with the parameters we found for the batch size, momentum (in the SGD) and the architecture of the neural network, we are able to find the optimal lambda and eta values. We will try to find those parameters that will get the highest accuracy score. We are setting the parameters in the following way:
\begin{lstlisting}[language = Python]
------------------
node_list = [20]*1
n_epochs =  200
batch_size = 14
gamma = 0.9
------------------
\end{lstlisting}
If we now insert those parameter values into \textbf{test 6} in \textbf{test\_project\_2.py}, we will get two plots (figure \ref{fig: classification_lmbda_eta_training} and figure \ref{fig: classification_lmbda_eta_testing}). 


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test6_nepochs_200_M_14_gamma_0.9_features_2_hiddennodes_20_hiddenlayer_1_actOUT_sigmoid_training.png}
    \captionof{figure}{data: training, accuracy of different lambda and eta values}
    \label{fig: classification_lmbda_eta_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test6_nepochs_200_M_14_gamma_0.9_features_2_hiddennodes_20_hiddenlayer_1_actOUT_sigmoid_test.png}
    \captionof{figure}{data: testing, accuracy of different lambda and eta values}
    \label{fig: classification_lmbda_eta_testing}
\end{figure}
\noindent
Figure \ref{fig: classification_lmbda_eta_training} and \ref{fig: classification_lmbda_eta_testing} tells us that a learning rate up towards 1e-3 is a good value, but the risk of getting exploding gradients will be higher with a greater learning rate. Therefore, we found it necessary to restrict the eta values that we are looping over (more than before). The learning rate is so sensitive because the momentum parameter (gamma) is at 0.9, which means that is will use 90\% of the last gradient value into the new iteration. The figures shows us that it doesn't depend that much on the parameter lambda, most  likely because we have chosen a simple architecture which is not likely to lead to an overfit. Although, we can still see some trends for lambdas work as an \textit{anti-overfitting} parameter.

\subsubsection{Activation functions}
%Activation functions: test all three, choose best
Since we are dealing with a binary classification case, the sigmoid function is obviously the best choice for the output layer. For the hidden layers, we study how the sigmoid function compares to the RELU and leaky RELU. First, we plot the accuracy score for the training data over iterations with the sigmoid activation layers. The following plots (fig. \ref{fig: classification_accuracy_over_time_sigmoid}, \ref{fig: classification_accuracy_over_time_RELU}, \ref{fig: classification_accuracy_over_time_leaky}) are achieved by running \textbf{Test 5} in the \textbf{test\_project\_2.py} - file, with the same parameters that we have been chosen as described in the previous subsection, and changing the hidden activation function between (respectively):

\begin{lstlisting}[language = Python]
--------------------------------------------------------
FFNN.set_activation_function_hidden_layers('sigmoid')
FFNN.set_activation_function_hidden_layers('RELU')
FFNN.set_activation_function_hidden_layers('Leaky_RELU')
--------------------------------------------------------
\end{lstlisting}



\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{sigma.png}
    \captionof{figure}{Accuracy of training data over iterations with the sigmoid function for hidden layers}
    \label{fig: classification_accuracy_over_time_sigmoid}
\end{figure}

\noindent
We create the same plot for the RELU function for the hidden layers. We have changed the $\gamma$ parameter from 0.9 to 0.5 as this gives a better result:

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{accuracy_of_last_training_0.5_0.0008.png}
    \captionof{figure}{Accuracy of training data over iterations with the RELU function for hidden layers}
    \label{fig: classification_accuracy_over_time_RELU}
\end{figure}

\noindent
Finally we create the same plot for the leaky RELU with the same parameters as for the RELU:

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{stinky.png}
    \captionof{figure}{Accuracy of training data over iterations with the leaky RELU function for hidden layers}
    \label{fig: classification_accuracy_over_time_leaky}
\end{figure}

\noindent
We observe that all the activation functions for the hidden layers give an accuracy score of above 90$\%$, yet the leaky RELU function seems to coverge to an accurate model in fewer iteration than both the sigmoid and the normal RELU, hence it is the best choice for this model. 


\subsubsection{Comparison with Logistic Regression}
%Check for different data sizes, amount of features
We have ran tests for both our neural network and the linear regression code. Both methods are able to make good predictions, and we have tuned the parameters so that both models are able to achieve the same high accuracy score (of about 90 $\%$). Then we compare the amount of time for training each model requires. We observe that the neural network achieves 90 $\%$ accuracy in about half the time of the logistic regression code. This shows that in this particular case, the neural network is a better model, even for only two features. 



\section{Conclusion}
%Critical evaluation of all algorithms, regression vs neural network
Conclusions and perspectives

\section{Appendix}
Appendix with extra material

\section{Bibliography}

\end{document}