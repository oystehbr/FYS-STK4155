\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{esint}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{ dsfont }
\usepackage{hyperref}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[font={small,it}]{caption}
\usepackage{caption}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage[euler]{textgreek}
\graphicspath{{./plots/}}
\usepackage{biblatex}
\addbibresource{reff.bib}
\usepackage{caption}
\usepackage{subcaption}

\hypersetup{
    colorlinks=true,
    linkcolor=blue
}

\title{Project 2 FYS-STK4155}
\author{Sigurd Holmsen, Øystein Høistad Bruce}
\date{November 2021}

\begin{document}

\maketitle

% PICTURE
%\begin{figure}
%    \includegraphics[scale=0.6]{Project1/UiO.jpg}
%\end{figure}

\newpage

\tableofcontents
\newpage

% TODO: slides week 41 The bias weights b are often initialized to zero, but a small value like 0.01 ensures all neurons have some output which can be backpropagated in the first training cycle.

\section{Abstract}

An abstract where you give the main summary of your work

\section{Introduction}
% TODO: explain 
% TODO: Use Franke Frankfunction as in project 1 and using the breast cancer data from scikitlearn 
%Explain neural network, modelled after neurons in biology, motivate!


An introduction where you explain the aims and rationale for the physics case and what you have done. At the end of the introduction you should give a brief summary of the structure of the report

\section{Method}
Here, we explain the functions we have programmed and some concepts behind them. The functions take in input data $X$ and output data $y$, so data must be generated or extracted from somewhere before the models can be used. 
%TODO: explain assumptions for all models and data 
\subsection{Scaling of data}

\subsection{Gradient Descent}
The goal of gradient descent is to find parameters that minimizes a cost function. We have written a gradient descent function that takes in initial values of the parameters that will be tuned, a cost function, the learning rate $\eta$, a hyper-parameter $\lambda$ and the number of iterations, as well as input and output data $X, y$. The learning rate $\eta$ decides the length of each step in the search direction. In this method, we have used the python package autograd for finding the gradients of the cost function. We then use the gradients and the learning rate to converge towards the minimum of the cost function, after which the function returns the updated/ tuned parameters that's defining the model. The algorithm we used is explained in the next subsection. 

\subsubsection{Momentum}
We have used a momentum based gradient descent, which means the length of each step in the gradient descent depends on the size of the previous gradient. The point is to do longer steps when gradients are steeper, and smaller steps when the gradients are flatter. In this way, the gradient descent adjusts to the "terrain" of the cost function. The algorithm can be expressed as follows: 

\begin{align}
    v_t &= \gamma v_{t-1} + \eta_t \frac{d C}{d \theta} \\
    \theta_{t+1} &= \theta_t - v_t
\end{align}

where the parameter $\gamma \in [0, 1]$ is the amount of momentum we want to use (no momentum is 0). $v_t$ is the amount of change in the tuning parameter at time $t$. $\theta$ is the parameter we want to tune, and $\frac{d C}{d \theta}$ is the change in cost with respect to $\theta$.

\subsubsection{Stochastic}
We have made our gradient descent method allowing stochastic, meaning we do not necessarily compute the gradients for the entire data set $X, y$, but for a small, randomly chosen batch $X_k, y_k$. If we let the size of the mini-batches to be $M$, then we will have $m$ mini-batches, where $m=\frac{\text{number of data}}{M}$ (rounded down to nearest integer). We then select a random integer $k\in [0, 1, ... (m-1)]$, and select the k-th mini-batch:

\begin{align*}
    \text{}
    X_k &= X[k \cdot M:(k+1) \cdot M]\\
    y_k &= y[k \cdot M:(k+1) \cdot M]
\end{align*}
In this way, we have produced a randomly chosen mini-batch from $X, y$, and computed the gradient $\frac{\text{number of data}}{M}$ times. We also decide the number of epochs, that is how many times we will repeat this process. The goal of the stochastic gradient descent is to save computational power and time, as we compute fewer gradients. 

\subsection{Neural Network}
\subsubsection{Architecture}
We have made a neural network class. Regarding the network architecture, the class takes the following parameters: number of input nodes, number of output nodes and a list of hidden layers with corresponding nodes. Each element of the list of hidden layers is an integer representing the number of nodes of the related layer. This allows us to set up the network architecture however we would like, where number of nodes per layer can vary. For more information on how to initiate a class, please look at the README file. 
\\ \\
Then we initialize the weights using the normal distribution and the biases as small float numbers. The class also has parameters for the activation function for the hidden layers and for the output layer, where the sigmoid function is the default activation function. Then, the user can choose the values for the stochastic gradient descent. That includes the learning rate $\eta$, the regularization parameter $\lambda$, batch size and number of epochs in the SGD, and the hyper-parameter $\gamma$.
\\ \\
The program does not scale data, so this must be done beforehand if needed. For more information on how the code works, look at the comments and doc-strings in the code. 

\subsubsection{Regularization parameters}
Our network takes a regularization parameter $\lambda$ which adds a term to the cost function:
$\lambda  \left\lVert w \right\rVert^2_2$. This term is added in attempt to avoid an overfit of the model. This is implemented as a parameter in the Neural Network class as explained above. 
% TODO: vanishing gradients
% TODO: exploding gradients

\subsubsection{Hyperparameter}
The network has also a method for choosing the hyperparameter $\gamma$, which decides the amount of momentum applied to the gradient descent. This may help finding a minimum quicker. This is implemented as a parameter in the Neural Network class as explained above. % TODO: maybe refer to github instead?

\subsubsection{Cost Function}
When a neural network is initialized, one has to decide which cost function to use, as this will influence the gradient descent, or more specifically the back propagation algorithm. The cost function must be given in a method inside the class. It is important that the cost function can be interchanged, as the linear regression and classification cases use different cost functions. 


\subsubsection{Feed Forward}
% TODO: explain activation function
The feed-forward function takes a set of data and predicts the output given the model weights and biases. The data should be a matrix of dimension $(\#data, \#features)$. This means we are approximating several output values simultaneously. Each node is connected to all the nodes in the next layer, and every node has its own bias. After each node has received values from the previous nodes, multiplied with the weight matrix and added with the bias, it it sent through the hidden layer activation function. This value is sent to the nodes in the next layer, and this process is repeated until the output node(s) are reached. The output node(s) may have a different activation function than the hidden layers, or it may not have an activation function at all. 

\subsubsection{Back Propagation}
% TODO: do we need to explain the SGD again?
To tune our weight and bias parameters, we use a stochastic gradient descent (SGD). That means we randomly pick a batch from $X$, and the corresponding output values from $y$, and compute the gradients of the parameters from these. We use a momentum based SGD. To find the gradients for the weights and biases, we perform the back propagation algorithm. First, we choose a cost-function, for instance a variant of the mean square error, as a measurement of the quality of our model:

\begin{equation}
    MSE = \frac{1}{2n} \sum_i (y_i - \tilde{y}_i)^{2}
\end{equation}
Then we are interested in finding the gradients for a given layer $l$:

\begin{equation}
    \frac{\delta C}{\delta w^l_{ij}}, \, \frac{\delta C}{\delta b^l_j}
\end{equation}
According to the back propagation algorithm, this can be written as:

\begin{align}
    \frac{\delta C}{\delta w^l_{ij}} = \delta_j^l a_k^{l-1}, \, \frac{\delta C}{\delta b^l_j} = \delta_j^l
\end{align}

where

\begin{equation}
    \delta_j^l = \sum_k \delta_k^{l+1} w_{kj}^{l+1} f'(z^l_j)
\end{equation}
Here, $f$ is the activation function for the given layer. We can compute this if we know the value of $\delta^{l+1}$. We can compute, the last layer delta in such way:

\begin{equation}
    \delta^L = \frac{1}{2n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\end{equation}
So, we are able to find all the $\delta$ values starting backwards. After we have found all the gradients for both the weights and biases, we use this for the momentum based SGD in the same way as described in the section above. When the SGD has iterated through all the given epochs, the training of the model is complete. 

\subsubsection{Regression case}
% Scaling data may be smart to not set exploding gradients after first iteration (with random weights)
For the regression case, we will not need an activation function for the output layer. This is because we may want to fit any number from $-\infty$ to $\infty$, and then we do not want to put a boundary on the value of the output. We also use the MSE as the cost function, which is accounted for in the back propagation algorithm.
% TODO: the MSE or a variant of it

\subsubsection{Classification case}
% Scaling the data 
% Scaling data may be smart to not set exploding gradients after first iteration (with random weights)
For the classification case, we want the output to be interpreted as probabilities. Hence we want to limit the output between 0 and 1. Since we are studying a binary classification problem, we use the sigmoid function as the output layer activation function. We have used the log-loss cost function when deriving the back propagation algorithm:

\begin{equation}
    C(W) = - \sum_{i=0}^n y_i log(\hat{y}_i) + (1-y_i) log(1 - \hat{y}_i)
\end{equation}

When using the neural network, we have used cancer data from the Wisconsin studies, and have utilized principal component analysis to rule out the two most important features from the data set. Principal component analysis looks at the unit vectors (representing the features) with the greatest eigenvalues, and we then only send the chosen data into our neural network for the training. 


\subsection{Logistic Regression}
When performing the logistic regression, we have used the following cost function: 

\begin{equation}
    \Pi_{i=0}^n \, p(x_i=1|\beta)^{y_i} p(x_i=0|\beta)^{1-y_i}
\end{equation}

Where
\begin{equation}
    p(x=1|\beta) = \frac{e^{\beta_0 + \beta_1x_1 + ... + \beta_nx_n}}{1 + e^{\beta_0 + \beta_1x_1 + ... + \beta_nx_n}}
\end{equation}

Then, we perform the SGD as before, where we use the autograd package to find the gradients of the cost function. 

\subsection{The data we have used}
% TODO: more descibable
% TODO: mention that when we are looking at Franke Function data, then we use 100 datapoints -> and n_epochs = 300 (this i picked by have many iterations, but also computational friendly). noise to the FrankeFunction is 0.01
% TODO: explain that the regression cases in the results, then we are using the Franke Function and classification case then we are using breasts -> 2 components (argue why we are using 2, maybe show PCA-results. variance_explained_ratio??)
% TODO: explain why we sometimes get R2-score thats very wierd -> predicting same values
Obviously, every test requires data. For the testing of the stochastic gradient descent and the regression case of the neural network, we have generated data from the Franke function similarly as in project 1. That means we generate $x, y$ -pairs, both randomly between 0 and 1, and send them into the Franke function. A noise term is then added, which is a normally distributed value with a noise multiplier parameter $noise$.
\\ \\
For the classification case, we have used the Wisconsin cancer data found in the Sci-kit learn package $sklearn.datasets$. We can choose how much data we would like to collect. The data used in each test below will be specified. 



\newpage
\section{Results and discussion}

\subsection{Results of Stochastic Gradient Descent}
%TODO: compare with OLS, Ridge, Learning rates, batch size, hyperparameter etc.
For the OLS regression, we have compared the MSE for different learning rates, and we also compare to the analytical solution of the problem as the "correct solution". This are the results from the terminal:
\begin{lstlisting}[language = Python]
--------------------------------
MSE: 0.183828 (ANALYTICAL)
MSE: 0.188668 (NUMERICAL (eta =  1e+01))
MSE: 0.184138 (NUMERICAL (eta =  1e+00))
MSE: 0.183852 (NUMERICAL (eta =  1e-01))
MSE: 0.183933 (NUMERICAL (eta =  1e-02))
MSE: 0.226785 (NUMERICAL (eta =  1e-03))
MSE: 0.262431 (NUMERICAL (eta =  1e-04))
MSE: 0.267412 (NUMERICAL (eta =  1e-05))
MSE: 0.267925 (NUMERICAL (eta =  1e-06))
--------------------------------
\end{lstlisting}

We observe that the optimal learning rate is $\eta = 10^-1$ for this case, as this gives the lowest MSE which is also closest to the one for the analytical solution. 
\\ \\
We make a graph comparing the accuracy of our model as a function of learning rate and hyper-parameter $\lambda$ for Ridge regression.

% Is it correct place
\begin{figure}[htbp]
\centerline{\includegraphics[width=10cm, height=8cm]{training_acc.png}}
\caption{Plot of the accuracy of model as function of learning rate $\eta$ and hyper-parameter $\lambda$}
\label{fig}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=10cm, height=8cm]{testing_acc.png}}
\caption{Plot of the accuracy of model as function of learning rate $\eta$ and hyper-parameter $\lambda$}
\label{fig}
\end{figure}

For these plots we use the learning rate we found above of $10^{-1}$ We observe that the results are sensitive to the learning rate $\eta$, based on figure 1 and 2. A too large learning rate may cause the gradient descent to miss the global minimum we are trying to find, while a too small learning rate may not converge quickly enough to the minimum. When comparing different learning rates to each other, we have used static learning rates to better see the difference in results. We also see that a too high hyper-parameter $\lambda$ will unnecessarily punish good fits while too low $\lambda$ will fail to punish overfitting. We can clearly see that the accuracy of the model is more sensitive to the learning rate $\eta$ than the hyper-parameter $\lambda$. This indicates that we should be careful when choosing the learning rate, as it may greatly change the resulting model.
\\ \\
We have also observed that there is an optimal mini-batch size as well:
\\
\begin{lstlisting}[language = Python]
--------------------------------
MSE: 0.253460 (NUMERICAL (batch size = 1))
MSE: 0.253146 (NUMERICAL (batch size = 5))
MSE: 0.252790 (NUMERICAL (batch size = 10))
MSE: 0.253233 (NUMERICAL (batch size = 30))
MSE: 0.253524 (NUMERICAL (batch size = 60))
MSE: 0.252884 (NUMERICAL (batch size = 100))
--------------------------------
Results from terminal, learning rate = 1e-1, gamma = 0.5, epochs = batch_size
\end{lstlisting}
\\
If the mini-batch size is too small, there may be too much randomness in the computed gradients to converge quickly, and if the size is too large, the problem of computing time arises, which defeats the purpose of the stochastic gradient descent. Hence a mini-batch size of around 10 seems reasonable. We have used an epoch size equal to the size of mini-batches so that the total amount of iterations are the same for each case. We note that we can always increase the number of epochs which will normally give a better result, but this is computationally expensive. 



\newpage
\subsection{Testing the neural network}
\subsubsection{Regression case}
% TODO: order: architecture, batch_size and gamma, lmbda and learning rate
%Mostly problem b
%Analysis of the regularization parameters (task b)
%Seaborn
We will use our neural network to predict the Franke function as in project 1. How we extracted our data is described in TODO 3.4.1. To make the model work as well as possible, we have to choose the correct parameters. We have made a plot showing the optimal learning rate and regularization parameters $\eta, \lambda$, where we are trying to achieve the best R2-score:\\

%seaborn - comment too high or too low parameters
\\\noindent
\textbf{First, we need to find the optimal architecture of the neural network}\\
\\We are finding the optimal architecture by looking at a seaborn plot with the number of layers and nodes at the x- and y-axis (respectively). To be able to create such a plot, we need to initialize some parameters. We started by setting the parameters in the following way:

\begin{lstlisting}[language = Python]
--------------------------------
n_epochs = 300
batch_size = 10
gamma = 0
lmbda = 0.0001
eta = 0.001
--------------------------------
\end{lstlisting}
With these values, we received the following plots as shown in figure \ref{fig: regression_architecture_training} and figure \ref{fig: regression_architecture_testing}, by using \textbf{test 9} in \textbf{test\_project\_2.py}. The plot tells us how we should build up our neural network by picking some architecture, a combination of the amount of nodes and layers, that gives us the highest R2-score. \\

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{test9_M_10_gamma_0_lmbda_0.0001_eta_0.001_training_1.png}
    \captionof{figure}{Shows the R2-score of different architecture of the neural network}
    \label{fig: regression_architecture_training}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{test9_M_10_gamma_0_lmbda_0.0001_eta_0.001_test_1.png}
    \captionof{figure}{Shows the R2-score of different architecture of the neural network}
    \label{fig: regression_architecture_testing}
\end{figure}
% TODO: comment the architecture, and refer to the plot with figure name, since it can be moved very easily
\\\noindent
By looking at the two figures, we can observe that the amount of nodes need to be (much) greater than the amount of layers for the neural network to be good as a model. We can also see that among the cells with the highest R2-score, there will be some "bad" ones also. Those cells (with R2-score around 0), have gone to a local minimum - where the model is predicting the same target values for all kind of input data. It is possible to avoid many of such occasions, by refreshing the weights and biases (start the algorithm over again), but it looks like we shouldn't do so (TODO). We tried to find a region where the architecture was kind of stable and worked good as a model, those values were 40 hidden nodes and 3 hidden layers. So, from now on we are going to evaluate the remaining parameters given this architecture. 
\\


\newpage
\\\noindent
\textbf{Secondly, we need to find the optimal batch size and momentum parameter}\\
\\Now we want to find the optimal batch size and momentum parameter by looking at a seaborn plot with the momentum parameter and batch size at the x- and y-axis (respectively). To be able to create such a plot, we need to  initialize some parameters (again). Now, we have optimized the architecture of the neural network (by last interpretations), and will be using those as initial values. So, the parameters we now set are:

\begin{lstlisting}[language = Python]
--------------------------------
node_list = [40]*3
n_epochs = 300
lmbda = 0.0001
eta = 0.001
--------------------------------
\end{lstlisting}
If we now insert those parameter values inside \textbf{test 10} in \textbf{test\_project\_2.py}, then we will get two plot (figure \ref{fig: regression_batchSize_gamma_training} and figure \ref{fig: regression_batch_Size_gamma_testing}). By looking at the plots, it seems like the most "stable" values (the models that are not predicting the same output) lies in the middle of the plot. We can see the same trend in the R2-score for both training and testing data. So, we have chosen to go further with a batch size of 16, and a gamma value of 0.4


\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{test10_lmbda_0.0001_eta_0.001_hiddennodes_40_hiddenlayer_3_training_1.png}
    \captionof{figure}{Shows the R2-score of different batch size and gamma values}
    \label{fig: regression_batchSize_gamma_training}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{test10_lmbda_0.0001_eta_0.001_40_hiddenlayer_3_test_1.png}
    \captionof{figure}{Shows the R2-score of different batch size and gamma values}
    \label{fig: regression_batch_Size_gamma_testing}
\end{figure}


\newpage
\\\noindent
\textbf{Last, we need to find the optimal hyperparameter lambda and learning rate}\\
% TODO: didn't finish this!!!!
\\Now we want to find the optimal batch size and momentum parameter by looking at a seaborn plot with the momentum parameter and batch size at the x- and y-axis (respectively). To be able to create such a plot, we need to  initialize some parameters (again). Now, we have optimized the architecture of the neural network (by last interpretations), and will be using those as initial values. So, the parameters we now set are:
We have made a plot showing the optimal learning rate and regularization parameters $\eta, \lambda$, where we are trying to achieve the best R2-score:

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{test4_M_16_gamma_0.4_hiddennodes40_hiddenlayers_3_training_1.png}
    \captionof{figure}{Shows the R2-score of different lambda and eta values}
    \label{fig: regression_lmbda_eta_training}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{test4_M_16_gamma_0.4_hiddennodes40_hiddenlayers_3_test_1.png}
    \captionof{figure}{Shows the R2-score of different lambda and eta values}
    \label{fig: regression_lmbda_eta_testing}
\end{figure}



TODO: comment plots
We observe that as a general rule, the amount of neurons per layer should be greater than the number of hidden layers. The optimal choice seems to be aroud XXX, so we will be using this architecture for the rest of the tests. A too high complexity in the model will be more  computationally expensive, and may also result in an overfit, while a too simple model seems to give an underfit. Finally, we choose the optimal batch size and epochs. 


\subsubsection{Comparison with Linear Regression}
%Observe: the neural network may depend on the inital weights and biases
%Linear regression not good for tuning many variables



\subsection{Activation functions}
\subsubsection{Sigmoid}
\subsubsection{the RELU}
\subsubsection{Leaky RELU}
\subsubsection{Soft max}

\subsection{Initializing of the weights and biases}
% Refreshing the weights and biases if some condition, why? Bad predictions
% Set biases to be different from zero

\subsection{Results of classification case}
When looking at the data form the Wisconsin studies, the neural network is able to achieve a high accuracy score (ca. 90$\%$), even when only looking at the two most important features from the principal component analysis. 
%test cancer data and analyze for different amount of features. 

\subsubsection{Comparison with Logistic Regression}
%Check for different data sizes, amount of features
We have ran tests for both our neural network and the linear regression code. Both methods are able to make good predictions, and we have tuned the parameters so that both models are able to achieve the same high accuracy score (of about 90 $\%$). Then we compare the amount of time for training each model requires. We observe that the neural network achieves 90 $\%$ accuracy in about half the time of the logistic regression code. This shows that in this particular case, the neural network is a better model, even for only two features. 




\section{Conclusion}
%Critical evaluation of all algorithms, regression vs neural network
Conclusions and perspectives

\section{Appendix}
Appendix with extra material

\section{Bibliography}

\end{document}