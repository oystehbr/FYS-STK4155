\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{esint}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{ dsfont }
\usepackage{hyperref}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[font={small,it}]{caption}
\usepackage{caption}
\usepackage{natbib}
\usepackage{graphicx}

\title{Project 1, FYS-STK4155}
\author{Sigurd Holmsen, Øystein Høistad Bruce}
\date{October 2021}

\begin{document}
\maketitle

\section*{Introduction to the code structure:}
We have decided to create a file, \textbf{helper.py}, which will store all the functions that are not used for just one exercise. For simplicity, we have created 6 files for the 6 different exercises. We will also provide some information about the different codes in the belonging exercise in this document. The code will also provide some helpful docstrings, so if something are unclear please check them out.\\
\\ We have created a helpful testing file, \textbf{test\_project\_1.py}, where you can test the code for the different exercises with different parameters. 

\section*{Exercise 1}
\textit{\textbf{Ordinary Least Square (OLS) on the Franke function}}
\subsection*{Nature of the problem}
We want to find a linear model which approximates a continuous function using polynomials with two arguments $(x, y)$ of high degrees. More precisely, we want to estimate parameters \(\beta\) such that 
\\
\begin{equation}
\label{egn: first_eq}
z_i = \beta _0 + \beta _1 x_i + \beta _2 y_i + \beta _3 x_iy_i + ...  + \beta_{(k-1)} x_i^m + \beta_ k y_i^m + \epsilon_i
\end{equation}
for all \(i\in {1, ..., m}\) where n is the number of data, and for a polynomial of degree m, where the number of $\beta$s,  $k$, depends on the polynomial degree $m$. \(\epsilon_i\) is the error term for each approximation, and we are looking for a model where the error is as small as possible. More precisely, we want the "Mean Square Error", which is given as the mean square of the difference between the data $y$ and the approximated data $\tilde{y}$:
\begin{equation}
\label{eqn: MSE}
    MSE = \frac{1}{n} \sum_i (y_i - \tilde{y}_i)^{2}
\end{equation}
to be small. This means our linear regression model is a good approximation to the function we want to approximate. 


\subsection*{Generating the data}
We have used the Franke function to generate the data used in the regression. We have added a noise variable to each datapoint which  is normally distributed as \(N(0,1)\). The code also includes a constant which scales each noise variable which can be changed by the user of the program. 

\subsection*{Perform a standard least square regression analysis}
To perform the standard least square regression, we first split the data into training and testing. The default testing size is 0.2 of the data. Then the data is scaled, before we compute the design matrix $\boldsymbol{X}$ which contains the variables used in the regression, and finally we use this along with the response variable $\boldsymbol{y}$ to compute the $\boldsymbol{\beta}$ values given by the formula for ordinary least squares regression:
\begin{equation}
    \boldsymbol{\beta} = (\boldsymbol{X^{T}} \boldsymbol{X})^{-1} \boldsymbol{X^{T}} \boldsymbol{y}
\end{equation}

\subsection*{Confidence intervals of the parameters $\boldsymbol{\beta}$}
We assume that the $\boldsymbol{\beta}$ values are normally  distributed, and $\sigma^2 (\beta_j) = \sigma^2 [(\boldsymbol{X}^T \boldsymbol{X})^-1 ]_{jj}$. Using this, we can 

\subsection*{Evaluating the Mean Squared error (MSE) and $R^{2}$ score}
We calculate the Mean Square Error as:
\begin{equation}
\label{eqn: MSE}
    MSE = \frac{1}{n} \sum_i (y_i - \tilde{y}_i)^{2}  %TODO, reference this from above
\end{equation}
where $\tilde{y}_i$ are the predicted outputs using our model, and use sci-kitlearn to evaluate the R2-score. 

\subsection*{Why do we scale our data?}
When using Ridge and Lasso regression, we include a term in the cost function which penalizes extreme values in order to avoid poor approximations e.g. overfitting. However, some features are often given in a different unit than other features, and the different units may differ greatly in magnitude. This will cause the penalizing team to penalize some features an unfair amount simply because of the magnitude of the unit it is given in. To fix this, we scale our data such that every feature may be assessed more equally. In our code, we have subtracted the mean of the data set for every feature as such:

\begin{equation}
    x^{(i)}_j \rightarrow x^{(i)}_j - \Bar{x}_j
\end{equation}

In some cases, you man also want to divide by the standard deviation, but in this case where it is not calculated we do not take it into account.





\subsection*{How to reproduce exercise 1 (Python):}


\subsection*{Final comments:}
\begin{enumerate}
    \item train test splitting
    
\end{enumerate}


\newpage
\section*{Exercise 2}
\textit{\textbf{Bias-variance trade-off and resampling techniques}}

% TODO: better title
% displaying only the test and training MSEs
\subsection*{Make a figure similar to Fig. 2.11}
\begin{figure}[h]
    \centering
    \includegraphics[width=12.7cm]{MSE_vs_Complexity_DP_180_d_14_OLS}
    \captionof{figure}{With some noise}
    \label{fig: MSE_vs_complexity}
\end{figure}\\

\newpage
% TODO: change title 
\subsection*{Math}
We want to find $\boldsymbol{\beta}$ by optimize the mean squared error, via the so-called cost function:

\begin{equation}
    C(\boldsymbol{X}, \boldsymbol{B})
        = \frac{1}{n} \sum_{i=0}^{n-1} (y_i - \tilde{y}_i)^{2}
        = \mathds{E} [(\boldsymbol{y} - \boldsymbol{\tilde{y}})^2]
\end{equation}

which we can rewrite to:\\

\begin{align*}
    \mathds{E} [(\boldsymbol{y} - \boldsymbol{\tilde{y}})^{2}] 
    &= \mathds{E} [(\boldsymbol{y} - \mathds{E} [\boldsymbol{\tilde{y}}] + \mathds{E} [\boldsymbol{\tilde{y}}] - \boldsymbol{\tilde{y}})^2]\\
    &= \mathds{E} [(\boldsymbol{y} - \mathds{E} [\boldsymbol{\tilde{y}}])^2] + 2 \mathds{E} [\boldsymbol{\tilde{y}} - \mathds{E} [\boldsymbol{\tilde{y}}]] \cdot \mathds{E} [\mathds{E} [\boldsymbol{\tilde{y}}] - \boldsymbol{y}] + \mathds{E} [(\boldsymbol{\tilde{y}} - \mathds{E}[\boldsymbol{\tilde{y}}])^2]\\
    &=^1 \mathds{E} [(\boldsymbol{f} + \boldsymbol{\epsilon} - \mathds{E}[\boldsymbol{\tilde{y}}])^2] + \mathds{E} [(\boldsymbol{\tilde{y}} - \mathds{E} [\boldsymbol{\tilde{y}}])^2]\\
    &= \mathds{E} [(\boldsymbol{f} - \mathds{E}[\boldsymbol{\tilde{y}}])^2] + 2\mathds{E}[\boldsymbol{f} - \mathds{E}[\boldsymbol{\tilde{y}}]] \cdot
    \mathds{E} [\boldsymbol{\epsilon}] + \mathds{E} [\boldsymbol{\epsilon}^{2}] + \mathds{E} [(\boldsymbol{\tilde{y}} - \mathds{E}[\boldsymbol{\tilde{y}}])^2]\\
    &=^2 \dfrac{1}{n} \sum_i (f_i - \mathds{E} [\boldsymbol{\tilde{y}}])^2 + \dfrac{1}{n} \sum_i (\tilde{y}_i - \mathds{E} [\boldsymbol{\tilde{y}}])^2 + \sigma ^2
\end{align*}
\underline{Hence}:
\begin{equation}
\label{eqn: bias_variance_equation}
    \mathds{E} [(\boldsymbol{y} - \boldsymbol{\tilde{y}})^2]
     = \frac{1}{n} \sum_{i} (f_i - \mathds{E}[\boldsymbol{\tilde{y}}])^{2}
     + \frac{1}{n} \sum_{i} (\tilde{y}_i - \mathds{E}[\boldsymbol{\tilde{y}}])^{2}
     + \sigma^{2}
\end{equation}
\underline{Comments}:
\begin{enumerate}
    \item $\mathds{E} [\boldsymbol{\tilde{y}} - \mathds{E} [\boldsymbol{\tilde{y}}]] = \mathds{E} [\boldsymbol{\tilde{y}}] - \mathds{E} [\boldsymbol{\tilde{y}}] = 0$
    \item $\mathds{E} [\boldsymbol{\epsilon}] = 0$, and $\mathds{E} [\boldsymbol{\epsilon}^2] = \mathds{E} [\boldsymbol{\epsilon}]^2 + \mathds{V} [\boldsymbol{\epsilon}] = \sigma^{2}$
\end{enumerate}\\
The equation (\ref{eqn: bias_variance_equation}) shows that the Mean Square Error can be rewritten as the sum of bias, variance and $\sigma^2$. The first term is bias, and this is an expression for how much our model is expected to deviate from the actual function we are approximating. It has to be noted that when we estimate this, we do not know the true function we are estimating, and we have to replace $f_i$ with our data $y_i$. The second term is the variation, and this shows how much our model varies from the expected computed model. This may become large if we have few data to train our model. When deciding what complexity to utilize when creating a model, one has to take into consideration both how it will affect the bias and the variance, as bias often decreases with complexity, while variation usually increases. 



\newpage
\subsection*{Perform a bias-variance analysis of the Franke function}

\begin{figure}[h]
    \centering
    \includegraphics[width=12.7cm]{bias_variance_boots_looping_degree_DP_250_d_8_OLS}
    \captionof{figure}{With some noise}
    \label{fig: bias_variance_OLS_boots}
\end{figure}\\

We plot the bias, variance and total MSE for the model as a function of complexity, and as seen in the plots, we recognize that the bias always decreases when the complexity increases. This makes sense, as the model we will not always be able to fit a complicated function using polynomials of low degrees, the variance for such models is always low in the results. When the complexity increases, we usually see the variance increase as well. These two effects makes it such that the MSE will often have a minimum when the complexity is not too high or too low, and hence we there is an optimal trade-off between bias and variance. However, if we increase the number of data points, we see that the variance will increase more slowly, and the optimal complexity will increase. Though it appears that you can always get rid of variance in the model by adding more data points, this will not necessarily be possible in real life scenarios, and this insures the importance of the bias-variance trade-off.



\subsection*{How to reproduce exercise 2 (Python):}
Reproduce figure 1
Reproduce figure 2

\subsection*{Final comments:}
\begin{enumerate}
    \item train test splitting
    
\end{enumerate}

\section*{Exercise 3}

\section*{Exercise 4}

\section*{Exercise 5}

\section*{Exercise 6}


\end{document}
