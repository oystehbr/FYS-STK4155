 \documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{esint}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{ dsfont }
\usepackage{hyperref}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[font={small,it}]{caption}
\usepackage{caption}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage[euler]{textgreek}
\graphicspath{{./plots/}}
\usepackage{biblatex}
\addbibresource{reff.bib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[font=small,labelfont=bf]{caption}
\setcounter{secnumdepth}{5}
\usepackage[autocite=footnote,notetype=foot+end,style=authortitle-ibid]{biblatex}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\title{Project 3 FYS-STK4155}
\author{Sigurd Holmsen, Øystein Høistad Bruce}
\date{November 2021}

\begin{document}

\maketitle

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=12cm]{CASE.NM.BILDE.jpg}
%     \caption{Sigurd Holmsen (far left), Øystein Bruce (far right)}
% \end{figure}

\newpage

\tableofcontents
\newpage


\section{Abstract}
\section{Introduction}
\section{Method}

\subsection{Measure the quality of a model}
\subsubsection{Confusion matrix}
\subsubsection{Data used}
For both data sets we have used, we have done a train-test split. We do not want to evaluate our model using the same data we used to train our model, as this will not reveal overfitted models. Therefore, we only use 80$\%$ of each data set for training, while the remaining $\%$ is used for testing the quality of the model after it is trained.


\subsection{Neural Network}
%We will give a brief summary of the method behind neural networks, and reference the details to our previous project. 
A neural network is inspired by how a biological brain processes signals. In a feed forward neural network, the input signal is fed through hidden layers before it reaches the output layer. Each layers consists of nodes which take in signals of nodes from the previous layer, multiplies it with a weight matrix, adds a bias parameter, then sends the signal through an activation function. When the input signal reaches the output layer, the model will have made a prediction. 
\\ \\
To make accurate predictions, the model will need to train. This is done through the back propagation algorithm, which updates the weights and biases using gradient descent. We have used a stochastic gradient descent, where a randomly chosen batch of the training data is chosen to compute the gradient with respect to the weights and biases. To compute the gradient, a cost function must be chosen. For a regular regression case this can for example be chosen as mean square error. For more information on the method of neural networks and all it's parameters, we reference our previous project: (referanse project 2)

\subsubsection{Multinomial Classification}
\subsubsection{(Cross-entropy) cost function}
\subsubsection{Softmax activation funciton}
% Explain: three output nodes, n_features input nodes
% What parts are the same as previous project?
\subsubsection{Tensorflow/Keras}
%Reference to the package, how we used it

\subsection{Logistic regression}
\subsubsection{Cost function}

\subsection{Decision tree}
% TODO: sebastian
\subsection{Random forest}
% TODO: sebastian

\section{Results and discussion}
\subsection{Introduction to the results}
In this section, we will refer to some tests we have made in the \textbf{test\_project\_3.py} file which will be found in the \textbf{project3} folder inside the github repository, attached at the end of the report (section \ref{sec_github_repository}). To have a great experience with the testing file, we recommend you to read the \textbf{README.md} file inside the \textbf{project3} folder. We will provide the necessary parameters for reproducing the tests while we present the results.
% TODO: maybe merge to one title??

\subsubsection{General comments on the results}
Here is a list of things to be aware of according to the results we are presenting:
\begin{enumerate}
    \item When we are comparing two parameters, we will look at the R2-score and accuracy score for the regression- and classification problem (respectively). This will very often be divided into two groups, training and testing data, where we often just provide results from the testing data.
    \item In our analysis, we had to initialize some parameters to be able to start tuning the first parameters. Whenever we achieved some results from a previous testing, we added the best parameters to the new testing. 
    \item The number of epochs we have chosen in the analysis isn't optimized for the best model. More iterations will, very often, achieve a better model. As a result we have chosen to go with some \textit{friendly} number of epochs to save computing expenses. We will show in section. % TODO: do we need to show this (as in last project)
    \item We need to make some initial guess of the parameters we want to tune with the SGD-algorithm. We have split the initial guess into:
    \begin{enumerate}
        \item linear regression case: $[0.0, 0.1, 0.1, ..., 0.1]$
        \item logistic regression case: $[0.1, ..., 0.1]$
    \end{enumerate}

\end{enumerate}



\subsection{Exploring different methods (classification problem)}
\subsubsection{Beans data set}
\label{sec_regression_data}
We have chosen to use a housing data set provided by The University of California at Irvine (UCI) \cite{data_set_beans}. \\

\noindent
The data consist of 13611 observations and 15 features. We will use all the data in our analysis. We have a classification data set where the targets are either $0, 1, ..., 6$  and represents different beans as shown below  \cite{data_set_beans}:\\

\begin{lstlisting}[language = Python]
-------------------------------------------------------
>> RUNNING TEST 9:
>> The distribution of the targets 
0:  0.149 (SEKER)
1:  0.097 (BARBUNYA)
2:  0.038 (BOMBAY)
3:  0.120 (CALI)
4:  0.142 (HOROZ)
5:  0.194 (SIRA)
6:  0.261 (DERMASON)
Total explained variance ratio (of 1 component):  1.000
Total explained variance ratio (of 2 component):  1.000
Total explained variance ratio (of 3 component):  1.000
-------------------------------------------------------
\end{lstlisting} 

\noindent
By running \textbf{test 9} in the \textbf{test\_project\_3.py}-file we achieve the results above, where we have converted the target classes into 7 different numerated classes. The beans in the data set is written behind the amount of occurrences it has throughout the whole data set.  \\

\noindent
We will use 3 features utilized by principal component analysis. We have explored the usage of 1 feature, since it returned such a high total explained variance ratio, but that would require more epochs in the fitting process, which we wanted to avoid. Principal component analysis (PCA) looks at the unit vectors (representing the features) with the greatest eigenvalues. We scale the data by the maximum of each input feature, so that the largest input value will be 1 for each feature. This is to adjust for different units for each feature.

\subsubsection{Neural network}
\label{sec_neural_network_regression}

% TODO: look over the data
We will start the analysis by studying the RELU as an activation function for the hidden layers and the softmax function for the output layer (since we work with a classification case). \\

\noindent
\textbf{First, we need to find the optimal architecture of the neural network}\\

\noindent
We are finding the optimal architecture by looking at a heatmap with the number of layers and nodes at the x- and y-axis (respectively). To be able to create such a plot, we need to initialize some parameters. We started by setting the parameters in the following way:

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
n_epochs = 80
batch_size = 500
gamma = 0.8
eta = 1e-2
lmbda = 1e-4
act_func = 'relu'
----------------------
\end{lstlisting}
With these values, we received the following plots as shown in figure \ref{fig: classification_architecture_testing}, by using \textbf{test 2} in \textbf{test\_project\_2.py}. The plot tells us how we should build up our neural network by picking some architecture, a combination of the amount of nodes and layers, that gives us the highest R2-score. \\

\noindent
By looking at the figure \ref{fig: classification_architecture_testing}, we can observe that the amount of nodes needs to be (much) greater than the number of hidden layers for the neural network to be good as a model. We can also see that among the cells with the highest R2-score, there will be some "bad" ones also. Those cells (with R2-score around 0), have (probably) gone to a local minimum - where the model is predicting the same target values for all kind of input data (as mention in section\ref{sec_weights_bias}). We tried to find a region where the architecture was kind of stable and worked good as a model, those values were 40 hidden nodes and 3 hidden layers. So, from now on we are going to evaluate the remaining parameters given this architecture. \\

\noindent
A too high complexity of the model will be more computationally expensive, and may also result in an overfit, while a too simple model seems to give an underfit.

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test2_M_500_gamma_0.8_lmbda_0.0001_eta_0.01_epochs_80_test_8.png}
    \captionof{figure}{Data: testing, accuracy of different architecture of the neural network}
    \label{fig: classification_architecture_testing}
\end{figure}

\noindent
\textbf{Secondly, we need to find the optimal batch size and momentum parameter} \\

\noindent
Now we want to find the optimal batch size and momentum parameter by looking at a heatmap with the momentum parameter and batch size at the x- and y-axis (respectively). To be able to create such a plot, we need to  initialize some parameters (again). Now, we have optimized the architecture of the neural network (by last interpretations), and will be using those as initial values. So, the parameters we now are producing plots with are:

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
eta = 1e-2
lmbda = 1e-4
act_func = 'relu'
hidden_nodes = 6
hidden_layers = 4
----------------------
\end{lstlisting}
If we now insert those parameter values inside \textbf{test 3} in \textbf{test\_project\_3.py}, then we will get two plots (figure \ref{fig: classification_batch_Size_gamma_testing_NN} - the one with testing data). By looking at the plots, it seems like the most "stable" values (the models that are not predicting the same output) lies in the middle of the plot. We can see the same trend in the R2-score for both training and testing data. So, we have chosen a batch size of 16, and a gamma value of 0.4.


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test3_lmbda_0.0001_eta_0.01_training_1.png}
    \captionof{figure}{Data: testing, accuracy score of different batch size and gamma values}
    \label{fig: classification_batch_Size_gamma_testing_NN}
\end{figure}

\noindent
\textbf{Last, we need to find the optimal hyperparameter lambda and learning rate} \\

\noindent
Now we want to find the optimal hyperparameter lambda and learning rate by making a grid search. Now, by the parameters we found for the batch size, momentum (in the SGD) and the architecture of the neural network, we are able to use those values for finding the optimal lambda and eta values. We will try to find the parameters, $\eta$ and $\lambda$, with the greatest R2-score. We are setting the parameters in the following way:

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
n_epochs = 80
gamma = 0.9
batch_size = 100
act_func = 'relu'
hidden_nodes = 6
hidden_layers = 4
----------------------
\end{lstlisting}
If we now insert those parameter values to \textbf{test 4} in \textbf{test\_project\_3.py}, then we will get two plots (figure\ref{fig: regression_lmbda_eta_testing}). The figures tells us that the best learning rate, $\eta$, is around $10^{-3}$ and $10^{-4}$ and the most stable lambdas are between $10^{-3}$ and $10^{-6}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test4_nepochs_80_M_100_gamma_0.9_test.png}
    \captionof{figure}{data: testing, accuracy score of different lambda and eta values}
    \label{fig: classification_lmbda_eta_testing_NN}
\end{figure}

\noindent
We can see at fig.\ref{fig: classification_lmbda_eta_testing_NN} that for a greater $\lambda$ value, the model will not be as overfitted as it would be without such a parameter. Also if the learning rate exceeds $10^{-3}$, we observe occurrences of exploding- or vanishing gradients, which leads to a poor model. With a too low learning rate, we observe that in some cases the training have gone too slow to reach some well tuned parameters. This could be fixed by setting up the number of epochs, but this would require more computing power which is something we want to avoid if possible. We conclude from this that the optimal parameters are: $\eta = 10^{-3}$, $\lambda = 10^{-4}$, $\gamma = 0.4$, batch size = 14, hidden layers = 4, nodes in hidden layers = 40. \\

\noindent
\textbf{Summary}\\

\noindent
Now, we want to see the accuracy when running the tuned parameters. We do this by setting the following parameters into \textbf{test 1} in the file \textbf{test\_project\_3.py}:

\begin{lstlisting} [language = Python]
----------------------
n_components = 3
m_observations = 13611
n_epochs = 80
batch_size = 100
lmbda = 1e-4
eta = 1e-3
gamma = 0.9
hidden_nodes = 6
hidden_layers = 4
confusion_result = True
----------------------
\end{lstlisting}

\noindent
The accuracy of the optimized neural network is as follows (printed in the terminal with \textbf{test 1}):

\begin{lstlisting} [language = Python]
-----------------------------------------
Accuracy for training: 0.9114420062695925
Accuracy for testing: 0.9174258007640317
-----------------------------------------
\end{lstlisting}

\noindent
We have also, by the parameters above, set the $confusion\_result$ parameter to True. That is, we will be provided a confusion matrix (figure \ref{fig: classification_confusion_matrix_NN}) for testing data with the neural network algorithm. 

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test1_confusion_matrix_NN_optimal.png}
    \captionof{figure}{data: testing, confusion matrix}
    \label{fig: classification_confusion_matrix_NN}
\end{figure}

\noindent
The confusion matrix shows what the testing data is predicting, which is more information than just the accuracy score since the accuracy score provides just true or false on the data. By the fig. \ref{fig: classification_confusion_matrix_NN}, we can see that it fails more between the classes 5 and 6, which are the beans \textit{Sira} and \textit{Dermason}. The prediction are probably a little bit wrong on these nuts, since they look kind of similar:

\begin{figure}[H]
    \begin{minipage}{.4\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{WhitebeansSiratype.jpg}
      \captionof{figure}{Sira beans \cite{pic_beans_sira}}
      \label{fig: sira_beans}
    \end{minipage}%
    \begin{minipage}{.4\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{whole-dried-white-dermason-beans-close-up-full-frame-top-shot-phaseolus-BJ7GP5.jpg}
      \captionof{figure}{Dermason beans \cite{pic_beans_dermason}}
      \label{fig: dermason_beans}
    \end{minipage}
\end{figure}


\subsubsection{Logistic regression}
Logistic regression blablabla \\

\noindent
Now, we need to initialize some parameters to start optimizing. We insert the following parameters to \textbf{test 12} in \textbf{test\_project\_3.py}, then we will get figure\ref{fig: classification_batch_size_gamma_testing_logistic}:

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
eta = 1e-3
lmbda = 0
----------------------
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test12_gamma_0.8_lmbda_0_test_.png}
    \captionof{figure}{data: testing, accuracy score of different batch size and gamma values}
    \label{fig: classification_batch_size_gamma_testing_logistic}
\end{figure}

\noindent
The figure \ref{fig: regression_batch_size_gamma_testing_logistic}, shows us that the accuracy of the model is kind of indifferent regarding to the gamma and batch size parameter. We are going to go further with gamma = $0.8$ and batch size of $500$. \\

\noindent
Now, we need to initialize some parameters to start optimizing. We insert the following parameters to \textbf{test 13} in \textbf{test\_project\_3.py}, then we will get figure \ref{fig: classification_lmbda_eta_testing_logistic}:

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
n_epochs = 100
batch_size = 500
gamma = 0.8
----------------------
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test13_nepochs_100_M_500_gamma_0.8_features_3_test_14.png}
    \captionof{figure}{data: testing, accuracy score of different lambda and eta values}
    \label{fig: classification_lmbda_eta_testing_logistic}
\end{figure}

\noindent
In fig. \ref{fig: classification_lmbda_eta_testing_logistic}, we see that the accuracy does not depend that much on the regularization parameter lambda, but the learning rate get the best score with a value of $0.01$. 

\subsubsection{Decision tree}
There is just one parameter to optimize here, the depth of the decision tree. We increase the tree depth until we reach an accuracy of 1 for the training data, and then we compare with the accuracy for testing data to identify the optimal depth. \\

\noindent
We need to insert the following values into \textbf{test 6} in the \textbf{test\_project\_3}-file (to receive figure \ref{fig: classification_depth_testing_decision_tree}):

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
----------------------
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test6_accuracy_vs_decision_tree_classification_2.png}
    \captionof{figure}{data: testing, accuracy vs. depth (decision tree)}
    \label{fig: classification_depth_testing_decision_tree}
\end{figure}

\noindent
In fig. \ref{fig: classification_depth_testing_decision_tree}, we see that the depth giving the highest accuracy is around 5, and larger depths will cause an overfit. 


\subsubsection{Random forest}
There is just one parameter to optimize here, the depth of the decision tree. We increase the tree depth until we reach an accuracy of 1 for the training data, and then we compare with the accuracy for testing data to identify the optimal depth. \\

\noindent
We need to insert the following values into \textbf{test 8} in the \textbf{test\_project\_3}-file (to receive figure \ref{fig: classification_depth_testing_random_forest}):

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
----------------------
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test8_random_forest_accuracy_vs_complexity.png}
    \captionof{figure}{data: testing, accuracy vs. depth (random forest)}
    \label{fig: classification_depth_testing_random_forest}
\end{figure}

\noindent
In fig. \ref{fig: classification_depth_testing_random_forest}, we see that the depth giving the highest accuracy is around 6, and larger depths will cause an overfit. This depth is slightly high than for a normal decision tree, meaning the random forest can handle a slightly higher complexity before overfitting. It also seems to acheive a somewhat higher accuracy score, which is expected, as the random forest should give a better result than computing just one decision tree. 


\subsubsection{Comparison and discussion}

\subsection{Bias-variance trade-off (regression case)}
Now, we are going to look at the bias-variance trade-off for different machine learning algorithms. We will look at four different algorithms, which are neural network, linear regression, decision tree and random forest. 

% TODO: What is bias-variance tradeoff. Split the MSE, look over complexity of the model - the complexity needs to be established in the different methods

\subsubsection{Housing data set}
\label{sec_regression_data}
We have chosen to use a housing data set provided by the Sci-kit learn package $sklearn.datasets$ \cite{scikit_data_housing}. \\

\noindent
The data consist of 20640 observations and 8 features. We are reducing the data to 2000 observations in the sake of computing power and for reducing the time consumption. The target values lies between $0.15$ and $5$ and represents an average house value in units of $100000$. The average is being taken by a block group which typically has a population of $600$ to $3000$ people) \cite{scikit_data_housing}.\\

\noindent
We will use all the features utilized by principal component analysis. Principal component analysis (PCA) looks at the unit vectors (representing the features) with the greatest eigenvalues. We scale the data by the maximum of each input feature, so that the largest input value will be 1 for each feature. This is to adjust for different units for each feature. 

\subsubsection{Neural network}
\label{sec_neural_network_regression}
We need to optimize some parameters before we go over to look at the bias-variance trade-off. \\

\noindent
We have chosen to go for the hidden activation function RELU in our initial optimization. \\

\noindent
\textbf{First, we will optimize the momentum- and batch\_size - parameters} \\

\noindent
Now we want to find the optimal batch size and momentum parameter by looking at a heatmap with the momentum parameter and batch size at the x- and y-axis (respectively). To be able to create such a plot, we need to initialize some parameters. The parameters we have initialized are:

\begin{lstlisting}[language = Python]
---------------------
n_components = 8
m_observations = 2000
eta = 5e-4
lmbda = 1e-5
hidden_nodes = 60
hidden_layers = 7
act_func = 'relu'
---------------------

\end{lstlisting}
If we now insert those parameter values inside \textbf{test 22} in \textbf{test\_project\_3.py}, then we will get two plots. The plot of the R2-score for testing data are below (figure \ref{fig: regression_batch_Size_gamma_testing}). By looking at the plot, it seems like the optimal values are on the right hand size, so a greater momentum parameter and kind of indifferent by the batch\_size. We have chosen to go further with a batch size of 120, and a gamma value of 0.8 since those are giving one of the best R2-scores and a lower momentum are more safer/stable than a greater one.

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test22_M_400_gamma_0.9_lmbda_1e-05_eta_0.0005_test_3.png}
    \captionof{figure}{Data: testing, R2-score of different batch sizes and gamma values}
    \label{fig: regression_batch_Size_gamma_testing}
\end{figure}

\noindent
\textbf{Then, we need to find the optimal hyperparameter lambda and learning rate} \\

\noindent
Now we want to find the optimal hyperparameter lambda and learning rate by making a grid search. Now, by the parameters we found for the batch size, momentum (in the SGD) and initial values of the architecture of the neural network, we are able to use those values for finding the optimal lambda and eta values. We will try to find the parameters, $\eta$ and $\lambda$, with the greatest R2-score. We are setting the parameters in the following way:

\begin{lstlisting}[language = Python]
---------------------
n_components = 8
m_observations = 2000
gamma = 0.8
batch_size = 120
hidden_nodes = 60
hidden_layers = 7
act_func = 'relu'
epochs = 500
---------------------

\end{lstlisting}
If we now insert those parameter values to \textbf{test 23} in \textbf{test\_project\_3.py}, then we will get two plots (provided here is the testing data plot, figure \ref{fig: regression_lmbda_eta_testing}). The figures tells us that the best learning rate, $\eta$, is around $10^{-2}$ and $10^{-3}$ and the most stable lambdas are between $10^{-3}$ and $10^{-5}$.


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test23_nepochs_500_M_120_gamma_0.8_features_8_test_14.png}
    \captionof{figure}{data: testing, R2-score of different lambda- and eta values}
    \label{fig: regression_lmbda_eta_testing}
\end{figure}

\noindent 
We can see at fig. \ref{fig: regression_lmbda_eta_testing} that for a greater $\lambda$ value, the model will not be as overfitted as it would be without such a parameter. Also if the learning rate exceeds $10^{-2}$, we observe occurrences of exploding- or vanishing gradients, which leads to a poor model. With a too low learning rate, we observe that in some cases the training have gone too slow to reach some well tuned parameters. This could be fixed by setting up the number of epochs, but this would require more computing power which is something we want to avoid if possible. We conclude from this that the optimal parameters are: $\eta = 10^{-2}$, $\lambda = 10^{-4}$, $\gamma = 0.8$, batch size = 120. \\

\noindent
\textbf{Now, we are ready for doing the bias-variance trade off with the optimal values for the neural network} \\

\noindent
The complexity of a neural network can be regarded as the number of hidden layers, since the model is getting more complex when we add an additional layer (more parameters that needs to be tuned). \\

\noindent
We are using the optimal parameters established in the two last tests to study the bias-variance trade-off of the neural network. The values we are inserting into \textbf{test 21} in \textbf{test\_project\_3.py} are:

\begin{lstlisting}[language = Python]
---------------------
n_components = 8
m_observations = 2000
gamma = 0.8
batch_size = 120
eta = 1e-2
lmbda = 1e-4
hidden_nodes = 60
epochs = 500
act_func = 'relu'
---------------------
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test21_bias_variance_boots_NN.png}
    \captionof{figure}{data: testing; bias-variance trade-off (neural network)}
    \label{fig: bias_variance_neural_network_regression}
\end{figure}

\noindent
Figure \ref{fig: bias_variance_neural_network_regression} shows us the bias, variance and total MSE for the model as a function of complexity where we use the bootstrap resampling method for the predicted values. In the plot, we recognize that the bias always decreases when the complexity increases. This makes sense, as the model will not always be able to fit a complicated data set using a neural network of few hidden layers, the variance for such models is always low in the results. When the complexity increases, we usually see the variance increase as well. These two effects makes it such that the MSE will often have a minimum when the complexity is not too high or too low, and hence there is an optimal trade-off between bias and variance. However, if we increase the number of data points, we see that the variance will increase more slowly, and the optimal complexity will increase. Though it appears that you can always get rid of variance in the model by adding more data points, this will not necessarily be possible in real life scenarios, and this insures the importance of the bias-variance trade-off.

% TODO: look over above


\subsubsection{Linear regression}
We will look at the OLS-regression to analyse the bias-variance trade-off



\subsubsection{Decision tree}
In the decision tree algorithm, the max depth is the parameter that can represent the complexity of the model. This parameter is the most important to tune in this algorithm, and therefore we do not need any pre-tuning before we look at the bias-variance trade-off. \\

\noindent
We set these values into \textbf{test 24} in \textbf{test\_project\_3.py}:

\begin{lstlisting}[language = Python]
---------------------
n_components = 8
m_observations = 2000
max_degree = 20
n_bootstrap = 1000
---------------------
\end{lstlisting}

\noindent
and the test gives us the following plot (figure \ref{fig: decision_tree_bias_variance_regression})

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test24_decision_tree_2.png}
    \captionof{figure}{data: testing; bias-variance trade-off (decision tree)}
    \label{fig: decision_tree_bias_variance_regression}
\end{figure}

\noindent
The figure \ref{fig: decision_tree_bias_variance_regression} shows us that % TODO: above

\subsubsection{Random forest}
In the random forest algorithm, the max depth is the parameter that can represent the complexity of the model (the same as in the decision tree method). In this case, we have also another parameter which decides the amount of trees we will be using in the algorithm, this is by default set to 100 and we will be using this in our bias-variance trade-off analysis. \\

\noindent
We set these values into \textbf{test 25} in \textbf{test\_project\_3.py}:

\begin{lstlisting}[language = Python]
---------------------
n_components = 8
m_observations = 2000
max_degree = 20
n_bootstrap = 100
---------------------
\end{lstlisting}

the test gives us the following result (figure \ref{fig: random_forest_bias_variance_regression})

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test25_random_forest_1.png}
    \captionof{figure}{data: testing; bias-variance trade-off (Random forest)}
    \label{fig: random_forest_bias_variance_regression}
\end{figure}

\noindent
The figure \ref{fig: random_forest_bias_variance_regression} shows us that % TODO: above

\section{Conclusion}

\section{Github repository}
\label{sec_github_repository}
\href{https://github.com/oystehbr/FYS-STK4155/tree/main/project3}{Github REPO - project 3}

\section{Bibliography}
\printbibliography[heading=none] 

\end{document}