\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{esint}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{ dsfont }
\usepackage{hyperref}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[font={small,it}]{caption}
\usepackage{caption}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage[euler]{textgreek}
\graphicspath{{./plots/}}
\usepackage{biblatex}
\addbibresource{reff.bib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[font=small,labelfont=bf]{caption}
\setcounter{secnumdepth}{5}
\usepackage[autocite=footnote,notetype=foot+end,style=authortitle-ibid]{biblatex}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\title{Project 3 FYS-STK4155}
\author{Sigurd Holmsen, Øystein Høistad Bruce}
\date{November 2021}

\begin{document}

\maketitle

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{CASE-NM-2020.jpg}
    \caption{\_, Sigurd Holmsen, Øystein Høistad Bruce - source: \cite{case_nm_bilde}}
\end{figure}

\newpage

\tableofcontents
\newpage


\section{Abstract}
When given a data set to study, there is a plethora of methods in machine learning to choose from. In this project, we will compare some of them to find which one works best for a data set we have chosen. We will find the parameters which give the best results, and study which algorithm works best, as well as the pros and cons for each method. As an additional problem, we will study the bias-variance trade-off for different machine learning approaches, and explore what impact this analysis has when choosing a machine learning method.

\section{Introduction}
This report will cover both the main- and optional task. We will first look into the methods that we have used in this project, and will give a deeper explanation of the additional methods to the optional project. The main difference from the previous project is that we are looking at a multinomial classification problem, where the data set consists of 7 target classes. The data sets we are using in this project are described in section \ref{sec_classification_data} and \ref{sec_regression_data} for the classification- and regression data set (respectively). \\

\noindent
In this project, we have used Tensorflow when creating and training the neural network, scikitlearn for decision trees and random forest-algorithms, and our own logistic regression implementation from the last project \cite{project_2}. The logistic regression code has some new features, since it will now handle multiple target values (more information in section \ref{sec_logistic_regression_method}). \\

\noindent
Before we run tests and compare results, we will give a presentation of the methods we have used, both for the main problem and for the additional problem. Several methods are repeated from the previous project, and this will be referenced when relevant. We will also give a brief explanation of how we measure the performance of the models. \\

\noindent
When testing our models, we will firstly look at the classification data set. Test some machine learning algorithms, neural network, logistic regression, decision tree and random forest, do some comparing and discussion of the different algorithms. In each machine learning technique, we will do some commenting on the results we gets and try to optimize the different hyperparameters. \\

\noindent
Then we will test the regression data set. As in project 1 (\cite{project_1}), we are again interested in doing the bias-variance trade-off analysis, but now with different machine learning methods. We will do the neural network, decision tree and random forest where we need to conclude the complexity of the different algorithms. Before we do the bias-variance analysis, we need once again to optimize the different parameters used in the methods. \\

\noindent
Lastly, we will summarize out results and compare the performances of the models with pros and cons, both for the main problem and the additional one. We will give a critical assessment and link with existing literature, before concluding with our main findings. 

\noindent
The code is stored in the Github repository attached at the last page of the report (section \ref{sec_github_repository}). The main code in the repository is the \textbf{test\_project\_3} file, which contains all the tests we are running in the result section. 

\section{Method}

\subsection{Measure the quality of a model}
\subsubsection{Accuracy- and R2-score}
We have used the accuracy- and R2-score for the evaluation of the models, these measurements can your read more about in our last project \cite{project_2}.

\subsubsection{Confusion matrix}
The confusion matrix shows the ways in which your classification model
is confused when it makes predictions \cite{confusion_matrix}. The confusion matrix presents the number of correct and incorrect predictions, within a matrix where the entries are different classes. This measuring tool is a great way to get to know your model prediction, and will provide more information than the accuracy score which will only tell you a true-false statement of the prediction. \\

\noindent
Figure \ref{fig: confusion_matrix_example} is an example of a confusion matrix, where the diagonal refer to a correct predictions. The other entries shows if it was predicted wrong, what was the prediction. The actual values are along the y-axis and the predictions along the x-axis. 

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{confusion_matrix_example.png}
    \captionof{figure}{confusion matrix example}
    \label{fig: confusion_matrix_example}
\end{figure}

\noindent
Analysis of the confusion matrix could be by looking at the predictions of the wrong predicted values, and see if the target values have highly correlated properties, shape etc. For instance, if the target values are of a ranking from $1-10$, then a smaller "predicting error" distance is better than a greater. In such cases, there will probably be some good models even though the accuracy score is low.

\subsubsection{Data used}
For both data sets we have used, we have done a train-test split. We do not want to evaluate our model with the same data we used to train it, as this will not reveal overfitted models. Therefore, we only use 80$\%$ of each data set for training, while the remaining $\%$ is used for testing the quality of the model after it is trained. It's the result from the testing data we are most interested in. The details of the data sets will be explained in the results section (\ref{sec_classification_data} and \ref{sec_regression_data}).

\subsection{Neural Network}
%We will give a brief summary of the method behind neural networks, and reference the details to our previous project. 
A neural network is inspired by how a biological brain processes signals. In a feed forward neural network, the input signal is fed through hidden layers before it reaches the output layer. Each layers consists of nodes which take in signals of nodes from the previous layer, multiplies it with a weight matrix, adds a bias parameter, then sends the signal through an activation function. When the input signal reaches the output layer, the model will have made a prediction. 
\\ \\
To make accurate predictions, the model will need to train. This is done through the back propagation algorithm, which updates the weights and biases using gradient descent. We have used a stochastic gradient descent, where a randomly chosen batch of the training data is chosen to compute the gradient with respect to the weights and biases. To compute the gradient, a cost function must be chosen. For a regular regression case this can for example be chosen as the mean square error. 
\\ \\
For this project, we look at both a regression problem and a classification problem, and these two problems require slightly different neural networks. The regression problem will be similar to out previous project \cite{project_2}, but since we have more than two classes in the classification problem, some new concepts will be introduced below. For more information on the method of neural networks and stochastic gradient descent, we reference our previous project: \cite{project_2}. We have used "tensorflow" with "keras" to create instances of our neural networks. The documentation is referenced here: \cite{tf_documentation}

\subsubsection{Multinomial Classification}
For the first data set, we will utilize multinomal classification, which means the targets in the data are one of $n$ classes. To translate the different classes into usable data, each class instance is represented by it's own one-hot vector \cite{onehot}. The only non-zero element is placed correspondingly with the class:

\begin{align*}
    Class_0 &= (1, 0, ..., 0) \\
    Class_1 &= (0, 1, ..., 0) \\
    &... \\
    Class_{n-1} &= (0, 0, ..., 1)
\end{align*}

\noindent
This is done using the "to categorical" function from "keras" \cite{to_categorical}. The neural network will need $n$ output nodes to predict each one-hot vector, where the values of each output node can be interpreted as the probability of the input being of the corresponding class. To further separate the multi-class case from the binomial case we did in project 2, we need to introduce a cost function and an activation for the output layer. 

\subsubsection{(Cross-entropy) cost function}
As a cost function for the neural network, we will be using the cross-entropy cost function:

\begin{equation}
    C(\boldsymbol{\hat{y}}; \boldsymbol{y}) = - \sum_{i=0}^{n-1} log(\hat{y}_i) y_i
\end{equation}

\noindent
Where $\boldsymbol{\hat{y}}$, $\boldsymbol{y}$ is the predicted target and the actual target (respectively) of the neural network. This function can be interpreted as a maximum log likelihood with a negative sign, meaning the higher the likelihood, the lower the cost. 

\subsubsection{Softmax activation function}
Since we want to restrict our output layer so that it represents probabilities of each class, we need an activation function for the output layer that sums up to 1. We use the Softmax function:

\begin{equation}
    f(z_j) = \frac{e^{z_j}}{\sum_{i=0}^{n-1} e^{z_i}} 
\end{equation}

\noindent
After this is applied to every node of the output layer, we have our model prediction. 

\subsection{Logistic regression}
\label{sec_logistic_regression_method}
Logistic regression for multi-class output uses the following model:
\begin{equation}
    p(y_j = 1 ;\boldsymbol{x}, \boldsymbol{\boldsymbol{\beta}}) = \frac{e^{(x_j)^T \boldsymbol{\beta_c}}}{\sum_{i=0}^{n-1} e^{(x_i)^T \boldsymbol{\beta_{i}}}} 
\end{equation}

\noindent
Where $p(y_{j} = 1)$ is the probability that a given input data will be part of class $class_j$, and each class $j=0,...,n-1$ will have its own $\boldsymbol{\beta}$ parameter $\boldsymbol{\beta_{j}}$. To find the best $\boldsymbol{\beta}$ parameter values for a given data set, we use stochastic gradient descent. 

\subsubsection{Cost function}
%Cost using the p function above. 
The cost function used in the gradient descent is the following:

\begin{equation}
    C(\boldsymbol{\beta}, \boldsymbol{x}, \boldsymbol{y}) = - \sum_{i=0}^{n-1} log \left(p(y_i = 1; \boldsymbol{x}, \boldsymbol{\beta}) \right) \, y_i
\end{equation}

\noindent
Again, the optimal $\boldsymbol{\beta}$ parameters will minimize the negative log likelihood function. For more information on logistic regression, we reference our previous project: \cite{project_2}.

\subsection{Decision tree}
\label{sec_decision_tree_theory}
% TODO: nevn at du kan få 100% accuracy when increasing accuracy in this method
% Kanskje forklar forskjellen mellom regression og classification problem for decision tree
% TODO: sebastian
\subsection{Random forest}
\label{sec_random_forest_theory}
% TODO: sebastian

\subsection{Bias-variance trade-off}
\label{sec_bias_variance_method}
When we study how the complexity affects the accuracy of a model, it is useful to look at the bias-variance trade-off. We often measure the mean square error of a model to determine how well it performs. The mean square error can be re-written as follows \cite{project_1}:

\begin{equation}
\label{eqn: bias_variance_equation}
    \mathds{E} [(\boldsymbol{y} - \boldsymbol{\tilde{y}})^2]
     = \frac{1}{n} \sum_{i} (y_i - \mathds{E}[\boldsymbol{\tilde{y}}])^{2}
     + \frac{1}{n} \sum_{i} (\tilde{y}_i - \mathds{E}[\boldsymbol{\tilde{y}}])^{2}
     + \sigma^{2}
\end{equation}

\noindent
Here, the first term is the expected square difference between the observed data and the expected model $E(\boldsymbol{\tilde{y}})$. This can be interpreted as the bias in the model, as this is the error of the expected,  "perfect" model. The second term is the expected square difference between the actual model $\boldsymbol{\tilde{y}}$ and the expected model $E(\boldsymbol{\tilde{y}})$, which is the variance of the model. If the bias is high, it means the model is not complex enough to predict the target data, and this would likely be an underfit. If the variance of the model is high, we likely have an overfit. Though it appears that you can always get rid of variance in the model by adding more data points, this will not necessarily be possible in real life scenarios, and this insures the importance of the bias-variance trade-off.

\subsubsection{Bootstrap as resampling}
To be able to get the estimates above, we need to do some resampling method. We have chosen to go for the boostrap method for the resampling process. We are using the independent boostrap method, since we do not have a probability distribution of the outputs. The bootstrap method goes as follow:

\begin{enumerate}
    \item Pick $n$ numbers for the observed variables with replacement, and save the result.
    \item Now, use the saved result from last step to compute the desired estimate. 
    \item Repeat the process (1-2), $k$ times
\end{enumerate}

\noindent
The method provided is inspired by \cite{bootstrap}.

\newpage
\section{Results and discussion}
\subsection{Introduction to the results}
In this section, we will refer to some tests we have made in the \textbf{test\_project\_3.py} file which will be found in the \textbf{project3} folder inside the github repository, attached at the end of the report (section \ref{sec_github_repository}). To have a pleasant experience with the testing file, we recommend you to read the \textbf{README.md} file inside the \textbf{project3} folder. We will provide the necessary parameters for reproducing the tests while we present the results.
% TODO: maybe merge to one title??

\subsubsection{General comments on the results}
Here is a list of things to be aware of according to the results we are presenting:
\begin{enumerate}
    \item When we are comparing two parameters, we will look at the R2-score and accuracy score for the regression- and classification problem (respectively). The tests inside the \textbf{test\_project\_3.py} file will often contain the result for both training and testing data, but we have provided the testing data results in this report (since this is the most important one).
    \item In our analysis, we had to initialize some parameters to be able to start tuning the first parameters. Whenever we achieved some results from a previous testing, we added the best parameters to the new testing. 
    \item The number of epochs we have chosen in the analysis isn't optimized for the best model. More iterations will, very often, achieve a better model. As a result we have chosen to go with some \textit{friendly} number of epochs to save computing expenses.
    \item We need to make some initial guess of the parameters we want to tune with the SGD-algorithm. The initial guess for the logistic regression case is $\beta = [0.1, ..., 0.1]$.
    \item While optimizing the batch size for the neural network, we have decided to set the number of epochs to a scalar times the batch size. This adjustment is to make the number of iterations in the SGD algorithm equal for each batch size. 
\end{enumerate}


\subsection{Exploring different methods (classification problem)}
\subsubsection{Beans data set}
\label{sec_classification_data}
We have chosen to use a data set provided by The University of California at Irvine containing data on different beans (UCI) \cite{data_set_beans}. \\

\noindent
The data consist of 13611 observations and 15 features. We will use all the data in our analysis. We have transformed the classification data set to targets that are either $0, 1, ..., 6$  and represents the different beans shown below: \\

\begin{lstlisting}[language = Python]
-------------------------------------------------------
>> RUNNING TEST 9:
>> The distribution of the targets 
0:  0.149 (SEKER)
1:  0.097 (BARBUNYA)
2:  0.038 (BOMBAY)
3:  0.120 (CALI)
4:  0.142 (HOROZ)
5:  0.194 (SIRA)
6:  0.261 (DERMASON)
Total explained variance ratio (of 1 component):  1.000
Total explained variance ratio (of 2 component):  1.000
Total explained variance ratio (of 3 component):  1.000
-------------------------------------------------------
\end{lstlisting} 

\noindent
By running \textbf{test 9} in the \textbf{test\_project\_3.py}-file we achieve the results above, where we have converted the target classes into 7 different numerated classes. The output in the terminal shows us the beans name and the distribution of the beans in the data set.  \\

\noindent
We will use 3 features utilized by the principal component analysis. We have explored the usage of 1 feature, since it returned such a high total explained variance ratio, but that would require more epochs in the fitting process, which we wanted to avoid. Principal component analysis (PCA) looks at the unit vectors (representing the features) with the greatest eigenvalues. \\

\noindent
We scale the data by the maximum of each input feature, so that the largest input value will be 1 for each feature. This is to adjust for different units for each feature. We have chosen to analyze this data with three different algorithms: neural network, logistic regression and decision tree. 

\subsubsection{Neural network}
\label{sec_neural_network_regression}

Since the data set has seven classes, we need a multinomial classification neural network as described above. We will start the analysis by studying the RELU function as an activation function. We will in the following test find parameters that give the best results (accuracy score) for the neural network. \\

\noindent
\textbf{First, we need to find the optimal architecture of the neural network}\\

\noindent
We will produce a heatmap with the number of layers and nodes at the x- and y-axis respectively. To be able to create such a plot, we need to initialize some parameters. We started by setting the parameters in the following way:

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
n_epochs = 80
batch_size = 500
gamma = 0.8
eta = 1e-2
lmbda = 1e-4
act_func = 'relu'
----------------------
\end{lstlisting}
The learning rate $\eta = 0.01$ is often a good initial guess. With these values, we received the following plot as shown in figure \ref{fig: classification_architecture_testing}, by using \textbf{test 2} in \textbf{test\_project\_3.py}. The plot tells us how we should build up our neural network by picking some architecture, a combination of the amount of nodes and layers, that gives us the highest R2-score. \\

\noindent
By looking at figure \ref{fig: classification_architecture_testing}, we can see a region with an accuracy of about $0.9$ and also some cells with a poor accuracy. These models are probably too simple to be accurate.  We tried to find a region where the architecture gave stable results, we picked 14 hidden nodes and 4 hidden layers. So from now on we are going to evaluate the remaining parameters given this architecture. \\

\noindent
A too high complexity of the model will be more computationally expensive, and may also result in an overfit, while a too simple model seems to give an underfit. So, we are picking the architecture with that in mind. 

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test2_M_500_gamma_0.8_lmbda_0.0001_eta_0.01_epochs_80_test_8.png}
    \captionof{figure}{Data: testing, accuracy of different architecture of the neural network}
    \label{fig: classification_architecture_testing}
\end{figure}

\noindent
\textbf{Batch size and momentum parameter} \\

\noindent
Now we want to find the optimal batch size for the SGD and momentum parameter by looking at a heatmap with the momentum parameter and batch size at the x- and y-axis respectively. To be able to create such a plot, we need to initialize some parameters (again). Now, we have optimized the architecture of the neural network (by last interpretations), and will be using those as initial values. So, the parameters we now are producing plots with are:

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
eta = 1e-2
lmbda = 1e-4
act_func = 'relu'
hidden_nodes = 14
hidden_layers = 4
----------------------
\end{lstlisting}
If we now insert those parameter values inside \textbf{test 3} in \textbf{test\_project\_3.py}, we will get two plots (figure \ref{fig: classification_batch_Size_gamma_testing_NN} - the one with testing data). By looking at the plot, it seems like the most "stable" values lies around a gamma value of 0.9. The accuracy looks independent of the batch size, so we will choose 100 to save compute expenses. 

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test3_lmbda_0.0001_eta_0.01_test_1.png}
    \captionof{figure}{Data: testing, accuracy score of different batch size and gamma values}
    \label{fig: classification_batch_Size_gamma_testing_NN}
\end{figure}

\noindent
\textbf{The hyperparameters learning rate and lambda} \\

\noindent
Now we want to find the optimal hyperparameter $\lambda$ and learning rate $\eta$ by making a grid search. Now, by the parameters we found for the batch size, momentum (in the SGD) and the architecture of the neural network, we are able to use those values for finding the optimal $\lambda$ and $\eta$ values. We will try to find the parameters $\eta$ and $\lambda$ that give the best accuracy score. We are setting the parameters in the following way:

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
n_epochs = 80
gamma = 0.9
batch_size = 100
act_func = 'relu'
hidden_nodes = 14
hidden_layers = 4
----------------------
\end{lstlisting}
If we now insert those parameter values to \textbf{test 4} in \textbf{test\_project\_3.py}, then we will get this plot (figure \ref{fig: classification_lmbda_eta_testing_NN}). The figure tells us that the best learning rate, $\eta$, is around $10^{-2}$ or $10^{-3}$ and the most stable lambdas are between $10^{-3}$ and $10^{-7}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test4_nepochs_80_M_100_gamma_0.9_test.png}
    \captionof{figure}{data: testing, accuracy score of different lambda and eta values}
    \label{fig: classification_lmbda_eta_testing_NN}
\end{figure}

\noindent
We can see from fig. \ref{fig: classification_lmbda_eta_testing_NN} that the results are mostly independent of the $\lambda$ value, so we will go for a $\lambda = 10^{-5}$. If the learning rate exceeds $10^{-2}$, we observe that the model is not as stable. With a too low learning rate, we observe that in some cases the training have gone too slow to reach well tuned parameters. This could be fixed by setting up the number of epochs, but this would require more computing power which is something we want to avoid if possible. \\

\noindent
We want to find the optimal activation function for the hidden layers. So far we have tested with the RELU function. In plot \ref{fig: activation_function_sigmoid} we have switched to the sigmoid activation function where all other parameters stay the same. We observe that it does not achieve a better result than the RELU function. Hence we use the RELU.

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test4_nepochs_20_M_100_gamma_0.9_test2.png}
    \captionof{figure}{data: testing, accuracy score of different lambda and eta values (activation function: sigmoid)}
    \label{fig: activation_function_sigmoid}
\end{figure}

\noindent
\textbf{Summary}\\

\noindent
We conclude from the optimization routine that the optimal parameters are: $\eta = 10^{-2}$, $\lambda = 10^{-5}$, $\gamma = 0.9$, batch size = 100, hidden layers = 4, nodes in hidden layers = 14. \\

\noindent
We want to look at the accuracy when running the optimal parameters. We do this by setting the following parameters into \textbf{test 1} in the testing file.

\begin{lstlisting} [language = Python]
----------------------
n_components = 3
m_observations = 13611
n_epochs = 30
batch_size = 100
lmbda = 1e-5
eta = 1e-2
gamma = 0.9
hidden_nodes = 14
hidden_layers = 4
confusion_result = True
----------------------
\end{lstlisting}

\noindent
The accuracy of the optimized neural network is as follows (the output in the terminal by running \textbf{test 1}):

\begin{lstlisting} [language = Python]
-----------------------------------------
>> RUNNING TEST 1 <<
Accuracy for training: 0.9114420062695925
Accuracy for testing: 0.9174258007640317
-----------------------------------------
\end{lstlisting}

\noindent
Our network is able to predict the class of $91\%$ of the testing data. We have also, by the parameters above, set the $confusion\_result$ parameter to True. That is, we will be provided a confusion matrix (figure \ref{fig: classification_confusion_matrix_NN}) for the testing data with the neural network algorithm.
\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test1_confusion_matrix_NN_optimal.png}
    \captionof{figure}{data: testing, confusion matrix}
    \label{fig: classification_confusion_matrix_NN}
\end{figure}

\noindent
The confusion matrix shows what the model predicts on the testing data. The confusion matrix gives more information than just the accuracy score since the accuracy score provides just true or false on the data , while the confusion matrix tells us what the wrong predictions were. By fig. \ref{fig: classification_confusion_matrix_NN}, it looks like it predicts (more often) wrong between the classes 5 and 6, which are the beans \textit{Sira} and \textit{Dermason}. The prediction are probably a little bit wrong on these nuts, since they look similar:

\begin{figure}[H]
    \begin{minipage}{.4\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{WhitebeansSiratype.jpg}
      \captionof{figure}{Sira beans \cite{pic_beans_sira}}
      \label{fig: sira_beans}
    \end{minipage}%
    \begin{minipage}{.4\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{whole-dried-white-dermason-beans-close-up-full-frame-top-shot-phaseolus-BJ7GP5.jpg}
      \captionof{figure}{Dermason beans \cite{pic_beans_dermason}}
      \label{fig: dermason_beans}
    \end{minipage}
\end{figure}


\subsubsection{Logistic regression}
The second method we have chosen to look at is the logistic regression. We will, as in the neural network algorithm, optimize the parameters used in the SGD-algorithm. \\

\noindent
\textbf{Batch size and momentum parameter} \\

\noindent
Now, we need to initialize some parameters to start optimizing. We insert the following parameters to \textbf{test 12} in \textbf{test\_project\_3.py}, then we will get figure \ref{fig: classification_batch_size_gamma_testing_logistic}:

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
eta = 1e-3
lmbda = 0
----------------------
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test12_gamma_0.8_lmbda_0_test_.png}
    \captionof{figure}{data: testing, accuracy score of different batch size and gamma values}
    \label{fig: classification_batch_size_gamma_testing_logistic}
\end{figure}

\noindent
The figure \ref{fig: classification_batch_size_gamma_testing_logistic}, shows us that the accuracy of the model is kind of indifferent regarding to both the gamma and batch size parameter. We are going to go further with gamma = $0.8$ and batch size of $500$. \\

\noindent
\textbf{The learning rate and lambda} \\

\noindent
We continue the optimization by insert the following parameters to \textbf{test 13} in \textbf{test\_project\_3.py}, then we will get figure \ref{fig: classification_lmbda_eta_testing_logistic}:

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
n_epochs = 100
batch_size = 500
gamma = 0.8
----------------------
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test13_nepochs_100_M_500_gamma_0.8_features_3_test_14.png}
    \captionof{figure}{data: testing, accuracy score of different lambda and eta values}
    \label{fig: classification_lmbda_eta_testing_logistic}
\end{figure}

\noindent
In fig. \ref{fig: classification_lmbda_eta_testing_logistic}, we see that the accuracy does not depend that much on the regularization parameter lambda, but the learning rate get the best score with a value of $0.01$. \\

\noindent
\textbf{Summary} \\

\noindent
Now, we want to see the accuracy and the confusion matrix, by running \textbf{test 11} in the file \textbf{test\_project\_3.py} with the tuned parameters:

\begin{lstlisting} [language = Python]
----------------------
n_components = 3
m_observations = 13611
n_epochs = 200
batch_size = 500
gamma = 0.8
eta = 1e-2
lmbda = 1e-5
confusion_result = True
----------------------
\end{lstlisting}

\noindent
The accuracy of the optimized logistic regression algorithm is as follows (printed in the terminal with \textbf{test 11}):

\begin{lstlisting} [language = Python]
-----------------------------------------
>> RUNNING TEST 11 <<
Training accuracy: 0.9065438871473355
Testing accuracy: 0.9062591830737584
-----------------------------------------
\end{lstlisting}

\noindent
The logistic regression achieves an accuracy score of around $90\%$. We have also, by the parameters above, set the $confusion\_result$ parameter to True. That is, we will be provided a confusion matrix (figure \ref{fig: classification_confusion_matrix_logistic}) for testing data with the logstic regression algorithm. 

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{tes11_confusion_matrix_logistic_optimal.png}
    \captionof{figure}{data: testing, confusion matrix}
    \label{fig: classification_confusion_matrix_logistic}
\end{figure}

\noindent
The confusion matrix looks similar in this case, as for the neural network method. % TODO: seb

\subsubsection{Decision tree}
There is just one parameter to optimize here, the depth of the decision tree. We increase the depth until we reach an accuracy of 1 for the training data, and then we compare with the accuracy for testing data to identify the optimal depth. \\

\noindent
We need to insert the following values into \textbf{test 6} in the \textbf{test\_project\_3}-file (to receive figure \ref{fig: classification_depth_testing_decision_tree}):

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
----------------------
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test6_accuracy_vs_decision_tree_classification_2.png}
    \captionof{figure}{data: testing, accuracy vs. depth (decision tree)}
    \label{fig: classification_depth_testing_decision_tree}
\end{figure}

\noindent
In fig. \ref{fig: classification_depth_testing_decision_tree} we see that the depth giving the highest accuracy is around 5. The accuracy for training data will converge towards $100\%$ as the complexity increases (as told in section \ref{sec_decision_tree_theory}). We observe that a larger depth will cause an overfit. \\

\noindent
\textbf{Summary:} \\

\noindent
Now, we want to see the accuracy and the confusion matrix, by running \textbf{test 5} in the file \textbf{test\_project\_3.py} with the tuned parameters:

\begin{lstlisting} [language = Python]
----------------------
n_components = 3
m_observations = 13611
max_depth = 5
confusion_result = True
----------------------
\end{lstlisting}

\noindent
The accuracy of the optimized decision tree algorithm is as follows (printed in the terminal with \textbf{test 5}):

\begin{lstlisting} [language = Python]
-----------------------------------------
>> RUNNING TEST 5:
Accuracy_train = 0.9042907523510971
Accuracy_test = 0.9021451660299735
-----------------------------------------
\end{lstlisting}

\noindent
The decision tree algorithm seems to also work well for this data set, we get an accuracy score of over $90\%$. We also get the following figure (\ref{fig: classification_confusion_matrix_decision_tree}) the confusion matrix for testing data with the decision tree algorithm. 

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test5_confusion_matrix_decision_tree_optimal.png}
    \captionof{figure}{data: testing, confusion matrix}
    \label{fig: classification_confusion_matrix_decision_tree}
\end{figure}

\noindent
The confusion matrix looks kind of similar to the two previous algorithms. 
% TODO: seb


\subsubsection{Random forest}
Lastly, we will apply a random forest algorithm to our data. There is just one parameter to optimize here, the depth of each decision tree. We increase the tree depth until we reach an accuracy of 1 for the training data, and then we compare with the accuracy for testing data to identify the optimal depth. \\

\noindent
We need to insert the following values into \textbf{test 8} in the \textbf{test\_project\_3}-file (to receive figure \ref{fig: classification_depth_testing_random_forest}):

\begin{lstlisting}[language = Python]
----------------------
n_components = 3
m_observations = 13611
----------------------
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test8_random_forest_accuracy_vs_complexity.png}
    \captionof{figure}{data: testing, accuracy vs. depth (random forest)}
    \label{fig: classification_depth_testing_random_forest}
\end{figure}

\noindent
In fig. \ref{fig: classification_depth_testing_random_forest}, we see that the depth giving the highest accuracy is around 6, and larger depths will cause an overfit. This depth is slightly higher than for a normal decision tree, meaning the random forest can handle a slightly higher complexity before overfitting. It also seems to achieve a somewhat higher accuracy score, which is expected, as the random forest should give a better result than computing just one decision tree. \\

\noindent
\textbf{Summary:} \\

\noindent
Now, we want to see the accuracy and the confusion matrix, by running \textbf{test 7} in the file \textbf{test\_project\_3.py} with the optimal parameter:

\begin{lstlisting} [language = Python]
----------------------
n_components = 3
m_observations = 13611
max_depth = 6
confusion_result = True
----------------------
\end{lstlisting}

\noindent
The accuracy of the optimized random forest algorithm is as follows (printed in the terminal with \textbf{test 7}):

\begin{lstlisting} [language = Python]
-----------------------------------------
>> RUNNING TEST 7:
Accuracy_train = 0.9141849529780565
Accuracy_test = 0.9133117837202468
-----------------------------------------
\end{lstlisting}

\noindent
We get an accuracy score of over $91\%$, slightly higher than the other methods. And we also get the following figure (\ref{fig: classification_confusion_matrix_random_forest}), the confusion matrix for testing data with the random forest algorithm. 

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test7_confusion_matrix_random_forest_optimal.png}
    \captionof{figure}{data: testing, confusion matrix}
    \label{fig: classification_confusion_matrix_random_forest}
\end{figure}

\noindent
The confusion matrix looks kind of similar to the other algorithms we have tested. % TODO: seb

\subsubsection{Comparison and discussion}
% TODO: seb (PROS and cons of the different methods??)

\newpage
\subsection{Bias-variance trade-off (regression case)}
Now, we are going to look at the bias-variance trade-off for three different machine learning algorithms: a neural network, decision tree and random forest. The bias variance trade-off analysis can be understood in section \ref{sec_bias_variance_method}. We will look at some errors against the complexity of the model, where the complexity is somewhat different from one method to another. 

\subsubsection{Housing data set}
\label{sec_regression_data}
We have chosen to use a housing data set provided by the Sci-kit learn package $sklearn.datasets$ \cite{scikit_data_housing}. \\

\noindent
The data consist of 20640 observations and 8 features. We reduced the data to 2000 observations for the sake of computing power and time consumption. The target values lie between $0.15$ and $5$ and represent an average house value in units of $100000$. The average is being taken by a block group which typically has a population of $600$ to $3000$ people) \cite{scikit_data_housing}. \\

\noindent
We will use all the features utilized by principal component analysis. Principal component analysis (PCA) looks at the unit vectors (representing the features) with the largest eigenvalues. \\

\noindent
We scale the data by the maximum of each input feature, so that the largest input value will be 1 for each feature. This is to adjust for different units for each feature. 

\subsubsection{Neural network}
\label{sec_neural_network_regression}
We need to optimize some parameters before we go over to the bias-variance trade-off analysis. \\

\noindent
\textbf{Batch size and momentum parameter} \\

\noindent
We have chosen to go for the hidden activation function RELU in our initial optimization. Now, we want to find the optimal batch size and momentum parameter by looking at a heatmap with the momentum parameter and batch size at the x- and y-axis (respectively). To be able to create such a plot, we need to initialize some parameters. The parameters we have initialized are:

\begin{lstlisting}[language = Python]
---------------------
n_components = 8
m_observations = 2000
eta = 5e-4
lmbda = 1e-5
hidden_nodes = 20
hidden_layers = 3
act_func = 'relu'
---------------------

\end{lstlisting}
If we now insert those parameter values inside \textbf{test 22} in \textbf{test\_project\_3.py}, then we will get two plots. The plot of the R2-score for testing data are below (figure \ref{fig: regression_batch_Size_gamma_testing}). By looking at the plot, it seems like the optimal values are on the right hand side, so a greater momentum parameter and it is closer to indifferent to the batch\_size. We have chosen to continue with a batch size of 120, and a gamma value of 0.8 since those give one of the best R2-scores and a lower momentum are more safer/stable than a higher one.

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test22_M_400_gamma_0.9_lmbda_1e-05_eta_0.0005_test_3.png}
    \captionof{figure}{Data: testing, R2-score of different batch sizes and gamma values}
    \label{fig: regression_batch_Size_gamma_testing}
\end{figure}

\noindent
\textbf{Hyperparameters learning rate and lambda} \\

\noindent
Now we want to find the optimal hyperparameter learning rate and lambda by making a grid search. Now, by the parameters we tuned for the batch size, momentum (in the SGD) and initial values of the architecture of the neural network, we are able to use those values for finding the optimal lambda and eta values. We will try to find the parameters, $\eta$ and $\lambda$, with the highest R2-score. We are setting the parameters in the following way:

\begin{lstlisting}[language = Python]
---------------------
n_components = 8
m_observations = 2000
gamma = 0.8
batch_size = 120
hidden_nodes = 60
hidden_layers = 7
act_func = 'relu'
epochs = 500
---------------------

\end{lstlisting}
If we now insert those parameter values into \textbf{test 23} in \textbf{test\_project\_3.py}, we get two plots (provided here is the testing data plot, figure \ref{fig: regression_lmbda_eta_testing}). The figure tells us that the best learning rate, $\eta$, is around $10^{-2}$ and $10^{-3}$ and the most stable lambdas are between $10^{-4}$ and $10^{-5}$.


\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test23_nepochs_500_M_120_gamma_0.8_features_8_test_14.png}
    \captionof{figure}{data: testing, R2-score of different lambda- and eta values}
    \label{fig: regression_lmbda_eta_testing}
\end{figure}

\noindent 
We can see at fig. \ref{fig: regression_lmbda_eta_testing} that for a higher $\lambda$ value, the model will not be as overfitted as it would be without such a parameter. Also if the learning rate exceeds $10^{-2}$, we observe occurrences of exploding- or vanishing gradients, which leads to a poor model. With a too low learning rate, we observe that in some cases the training has been too slow to reach well tuned parameters. This could be fixed by setting up the number of epochs, but this would require more computing power which is something we want to avoid if possible. \\

\noindent
We conclude from this optimizing that the optimal parameters are: $\eta = 10^{-2}$, $\lambda = 10^{-4}$, $\gamma = 0.8$, batch size = 120. \\

\noindent
\textbf{Now, we are ready for doing the bias-variance trade off with the optimal values for the neural network} \\

\noindent
The complexity of a neural network can be regarded as the number of hidden layers, since we introduce more parameters to tune with each new layer. \\

\noindent
We are using the optimal parameters established in the two last tests to study the bias-variance trade-off of the neural network. The values we are inserting into \textbf{test 21} in \textbf{test\_project\_3.py} are:

\begin{lstlisting}[language = Python]
---------------------
n_components = 8
m_observations = 2000
gamma = 0.8
batch_size = 120
eta = 1e-2
lmbda = 1e-4
hidden_nodes = 60
epochs = 500
act_func = 'relu'
---------------------
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test21_bias_variance_boots_NN.png}
    \captionof{figure}{data: testing; bias-variance trade-off (neural network)}
    \label{fig: bias_variance_neural_network_regression}
\end{figure}

\noindent
Figure \ref{fig: bias_variance_neural_network_regression} shows us the bias, variance and total MSE for the model as a function of complexity where we use the bootstrap resampling method for the predicted values. In the plot, we recognize that the bias will decreases when the complexity increases. This makes sense, as the model will not always be able to fit a complicated data set using a neural network of few hidden layers, the variance for such models is always low in the results of low complex models. When the complexity increases, we usually see the variance increase as well. These two effects makes it such that the MSE will often have a minimum when the complexity is not too high or too low, and hence there is an optimal trade-off between bias and variance.


\subsubsection{Decision tree}
In the decision tree algorithm, the depth is the parameter that can represent the complexity of the model. This parameter is the most important when tuning this algorithm, and therefore we do not need any pre-tuning before we look at the bias-variance trade-off. \\

\noindent
We set these values into \textbf{test 24} in \textbf{test\_project\_3.py}:

\begin{lstlisting}[language = Python]
---------------------
n_components = 8
m_observations = 2000
max_degree = 20
n_bootstrap = 1000
---------------------
\end{lstlisting}

\noindent
and the test gives us the following plot (figure \ref{fig: decision_tree_bias_variance_regression})

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test24_decision_tree_2.png}
    \captionof{figure}{data: testing; bias-variance trade-off (decision tree)}
    \label{fig: decision_tree_bias_variance_regression}
\end{figure}

\noindent
Figure \ref{fig: decision_tree_bias_variance_regression} shows us the same tendencies as in section \ref{sec_neural_network_regression}. The more complex model will result in an increasing variance and decreasing bias, and the optimal mean squared error is clearer in this plot. We can see that around a tree depth of 5 gives the lowest MSE, meaning it is the best trade-off between bias and variance. 
% TODO: above, more on bias-variance??

\subsubsection{Random forest}
In the random forest algorithm, we again view the tree depth as the parameter that represents the complexity of the model (the same as in the decision tree method). In this case, we have also another parameter which decides the amount of trees we will be using in the algorithm, this is by default set to 100 and we will be using this in our bias-variance trade-off analysis. \\

\subsubsection{Comments and discussion of the methods}
% TODO: seb

\noindent
We set these values into \textbf{test 25} in \textbf{test\_project\_3.py}:

\begin{lstlisting}[language = Python]
---------------------
n_components = 8
m_observations = 2000
max_degree = 20
n_bootstrap = 100
---------------------
\end{lstlisting}

the test gives us the following result (figure \ref{fig: random_forest_bias_variance_regression})

\begin{figure}[H]
    \centering
    \includegraphics[width=8.5cm]{test25_random_forest_1.png}
    \captionof{figure}{data: testing; bias-variance trade-off (Random forest)}
    \label{fig: random_forest_bias_variance_regression}
\end{figure}

\noindent
Figure \ref{fig: random_forest_bias_variance_regression} shows us that the random forest algorithm does not suffer as much in the variance for the increase of depth. This is because... TODO. The optimal complexity is harder to find in such cases, since the low increase in variance do not make a clear minimum of the mean squared error. 
% TODO: above

\section{Conclusion}

\section{Github repository}
\label{sec_github_repository}
\href{https://github.com/oystehbr/FYS-STK4155/tree/main/project3}{Github REPO - project 3}

\section{Bibliography}
\printbibliography[heading=none] 

\end{document}